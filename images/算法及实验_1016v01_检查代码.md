## 数字孪生赋能车载边缘计算中Age-of-Information-Aware协作数据卸载

Age-of-Information-Aware Collaborative Data Offloading in Digital Twin-Enabled Vehicular Edge Computing

### 编程区别之处

* ==微服务==，原来是的服务部署应该改为==孪生模型部署（不需要微服务的组合）==

  区别：原问题为车载服务的部署，修正后的问题为车辆的数字孪生模型的部署

  * 原问题中的微服务可以由多个子服务组成一个服务，一个用户可以请求任意的服务

  * 现在问题的孪生模型是一个整体，且一个车辆用户id只对应一个孪生模型

* 原设计中：计算卸载决策依赖于服务部署决策，即只有部署了相应的服务，才能执行对应的计算卸载$a_{ij}<=b_{sj}$;

  ==更正为如下==：

  * ==孪生模型的部署和数据卸载独立==，数据卸载可以在任何有计算能力的设备上计算，但计算的结果需要传输到孪生模型所在的边缘服务器执行模型更新

  * 空闲终端的协作：没有任务的终端可以协作计算，原设计要求部署了相应的服务才可以协作，现在孪生模型部署和卸载独立，任意空闲的终端都可以协作计算，但计算的结果需要传输到孪生模型所在的边缘服务器执行模型更新

    因此，AoI的计算上要考虑计算节点和孪生模型部署节点之间数据的传输

* 优化目标：原目标为最小化平均响应时延，更正为：最小化数据处理的平均峰值AoI
* **算法 2.3** 服务部署决策构建和计算卸载决策更新中，根据孪生模型更新请求的==时延需求==对 $\mathcal{S}^0_j$ 进行升序排序，生成服务优先级 $\mathcal{S}_j^*$，（==原来为服务请求次数，改为时延需要==）
* 在原来代码外层加上时隙迭代的循环，即 $\tau=0,1,\dots$



### 检查代码(==有截图表示有问题==)

- [x] 服务 -> 车辆孪生模型相关

  - 服务由微服务组成**已去掉**：`service`
  
  - 微服务相关**都已去掉**：微服务存储大小 `req_msR`的声明、定义、初始化；
  
  - 终端产生服务请求类型 `U1request` **已去掉**，微服务器已部署的服务类型 `U2service​` **已去掉**。
  
  - 服务流行度的 zipf 分布**已去掉**。
  
  - 服务已改为车辆孪生模型，定义了模型存储大小 `[1,10] GB​`
  
  - 模型存储大小目前取的是整数，**是否改成实数更符合实际些**：【已改，见main10_17_1.m】
    
    <div align="center" ><img src="https://cdn.jsdelivr.net/gh/wlchengg/PicBed@main/images_for_blogs/20241016232352.png" alt="20241016232352" width="75%" style="box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);border-radius:2px;"/></div>

- [x] 任务请求四元组相关
  
  - 上传数据实数`[1,5] Mbit`、算后数据比例 `0.1`、所需时钟频率整数`[200,400] CPU cycles/bit`、截止时延实数`[1,5] s`
  
- 终端数改为 600 -> 500

- 轨迹预测时长 5 s -> 4 s



- 时延计算的问题：
  
<div align="center" ><img src="https://cdn.jsdelivr.net/gh/wlchengg/PicBed@main/images_for_blogs/20241017003758.png" alt="20241017003758" width="100%" style="box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);border-radius:10px;"/></div>

- 判断条件已按照算法去掉`服务部署是否条件`：
  
<div align="center" ><img src="https://cdn.jsdelivr.net/gh/wlchengg/PicBed@main/images_for_blogs/20241017004245.png" alt="20241017004245" width="100%" style="box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);border-radius:10px;"/></div>

- 算法2.3 (代码3.1) 已将`排序条件`由服务请求次数更换为时延：
  
<div align="center" ><img src="https://cdn.jsdelivr.net/gh/wlchengg/PicBed@main/images_for_blogs/20241017010257.png" alt="20241017010257" width="100%" style="box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);border-radius:10px;"/></div>

- 代码3.3.1：卸载基站不需要是模型部署基站
  
<div align="center" ><img src="https://cdn.jsdelivr.net/gh/wlchengg/PicBed@main/images_for_blogs/20241017011451.png" alt="20241017011451" width="100%" style="box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);border-radius:10px;"/></div>

- 代码多余？
  
<div align="center" ><img src="https://cdn.jsdelivr.net/gh/wlchengg/PicBed@main/images_for_blogs/20241017012650.png" alt="20241017012650" width="100%" style="box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);border-radius:10px;"/></div>

- 代码6：统计总时延是似乎遗漏了**卸载结果传输到部署基站的传输时延** `timeOfMove`。现在优化目标是 AoI，计算公式里应该包含：卸载传输时延`timeOfUpload`、计算时延`time`、卸载结果传输到部署基站的传输时延`timeOfMove`。

<div align="center" ><img src="https://cdn.jsdelivr.net/gh/wlchengg/PicBed@main/images_for_blogs/20241017013633.png" alt="20241017013633" width="100%" style="box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);border-radius:10px;"/></div>

  或者目前变量就只记录卸载时延，然后另外定义变量来记录总时延，进而统计 AoI。~~另外，AoI 的值还需要加上时隙长度常数 $\Delta$，后续再补充定义。~~

## Related work 

【Data synchronization in vehicular digital twin network: A game theoretic approach】IEEE Trans. Wireless Commun., vol. 22, no. 11, pp. 7635–7647

【URLLC edge networks with joint optimal user association, task offloading and resource allocation: A digital twin approach,” IEEE Trans. Commun., vol. 70, no. 11, pp. 7669–7682】

【Edge intelligence-based ultra-reliable and low-latency communications for digital twin-enabled metaverse,” IEEE Wireless Commun. Lett., vol. 11, no. 8, pp. 1733–1737】

【“Digital twin empowered heterogeneous network selection in vehicular networks with knowledge transfer,” IEEE Trans. Veh. Technol., vol. 71, no. 11, pp. 12154–12168】

【Low-latency federated learning and blockchain for edge association in digital twin empowered 6G networks,” IEEE Trans. Ind. Informat., vol. 17, no. 7, pp. 5098–5107】



【**AoI计算卸载**】

【Joint near-optimal age-based data transmission and energy replenishment scheduling at wireless-powered network edge,” in Proc. IEEE INFOCOM】

【 AoI minimization charging at wireless-powered network edge,” in Proc. ICDCS, Jul. 2022, pp. 713–723.】

【Age-based scheduling for monitoring and control applications in mobile edge computing systems,” in Proc. IEEE INFOCOM.】

【Minimizing the age-of-critical-information: An imitation learning-based scheduling approach under partial observations,” IEEE Trans. Mobile Comput., vol. 21, no. 9, pp. 3225–3238】

【Schedule or wait: Age-minimization for IoT big data processing in MEC via online learning,” in Proc. IEEE INFOCOM】

【Optimizing information freshness through computation–transmission tradeoff and queue management in edge computing,” IEEE/ACM Trans. Netw., vol. 29, no. 2, pp. 949–963】



【**AoI in MEC**】

AoI-Aware User Service Satisfaction Enhancement in Digital Twin-Empowered Edge Computing

**Minimizing** Age of **Usage** Information **for Capturing Freshness and Usability** of **Correlated Data in** Edge **Computing Enabled IoT Systems** （TMC）



## 1. 系统模型与问题描述

### 1.1 系统模型

#### 1.1.1 车载边缘计算网络场景

随着车辆网设备的集成、边缘计算能力的增强和复杂的实时分析需求的不断发展，在车载边缘计算网络场景下，为了维护孪生模型的实时性，车辆终端可以将数据卸载到 MEC 服务器或邻近车辆终端执行，并在短时延内获得数据执行结果，然后将执行结果传输到车辆孪生模型所维护的边缘服务器，以更新车辆的数字孪生模型，实现车辆对象与其数字孪生体之间的实时同步。

==数字孪生车载边缘计算架构==如图1.1所示：（1）***\*云中心\****提供了强大的计算资源和存储资源，用于处理***\*大规模数据分析、高级模拟计算以及长期数据存储\****。（2）***\*孪生层\****用于创建和管理车辆及其运行环境的虚拟副本，分布式的孪生架构可以适应大规模车辆网络、保障数据处理的低延迟以及系统的可扩展性。孪生层允许对现实世界中的车辆和交通环境进行实时模拟、数据分析、状态检测以及决策制定。（3）实体层是车载边缘环境中物理实体和环境的集合，包括车辆、道路基础设施、交通信号系统、传感器网络、基站、路边单元等，为孪生层提供必要的实时数据和状态信息。

<div align="center" ><img src="https://cdn.jsdelivr.net/gh/wlchengg/PicBed@main/images_for_blogs/图1-系统架构-1009.png" alt="图1-系统架构-1009" width="75%" style="box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);border-radius:10px;"/><br><div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #777; font-size: 90%; padding: 1px;">图 1 系统架构</div></div>



车辆终端可将任务卸载到边缘服务器或微服务器，将非忙或空闲状态下的终端称之为协作者或微服务器，微服务器具有空闲的计算资源，可以协助邻近终端进行计算卸载。

为更好地描述车载边缘网络，下面将网络中的元素进行抽象并对其建模，本章涉及到的部分变量及定义总结如表 1.1 所示。

**表 1.1 变量总结表**

**重要变量替换：**

* 服务—孪生模型

* 移动终端—车辆

| 变量                         | 解释            | 变量                       | 解释                  |
|----------------------------|---------------|--------------------------|---------------------|
| $u_{i}\in U$ | ==车辆==终端集     | $a_{ij}\in\mathbf{a}$  | 卸载决策                |
| $b_{j}\in B$               | 微基站集          | $b_{ij}\in\mathbf{b}$   | 孪生模型部署决策          |
|     | ~~微服务集~~ | $c_{ij}$ $\in\mathbf{c}$ | 资源分配决策              |
|                    | ~~微服务 $m_i$ 的存储大小~~ | $R_{ij}^U,R_{ij}^D$      | $u_{i}$ 卸载上/下行传输速率   |
| $s_i$ $\in S$            | ==孪生模型集合==  | $T_{ij}^{UD}$          | $u_{i}$ 卸载任务总通信时延    |
| $r_i$            | ==孪生模型 $s_i$ 存储数据大小== | $T_i^L,T_{ij}^M$         | $u_{i}$ 本地/卸载执行的计算时延 |
| $Q_i$                      | $u_{i}$ 的==孪生模型更新请求== | $T_{ij}^{E}$             | $u_i$ 卸载数据总响应时延      |
| $F_j\in F$               | $b_j$ 的最大计算能力   | $R_j\in R$                | $b_j$ 的最大存储能力         |

车载边缘网络中部署多个具有通信、计算和存储能力的基站，其数量为 $M$，并用集合 $B=\{b_{1},b_{2},\cdots,b_{j},\cdots,b_{m}\}$ 表示，且 $| B|=M$。本文将车辆集合表示为 $U=\{u_{1},u_{2},\cdots,u_{i},\cdots,u_{n}\}$，$|U|=N$，每个车辆终端自身具有一定的计算能力，且需要==为每个车辆终端维护孪生模型==。
考虑到在实际环境中，不同时刻终端的状态是动态变化的，即每个时刻处于非忙碌状态和忙碌状态的终端数量是变化的，因此，本文将集合 $U$ 划分为集合 $U_{1}$ 和 $U_{2}$，即 $U=U_{1}\cup U_{2}$，$|U_1|=N_1$，$|U_2|=N_2=N-N_1$，注意，$N_1$ 和 $N_{2}$ 并非固定值，而是随时间不断变化。
$U_{1}$ 表示非空闲终端集合，即当 $u_i\in U_1$ 时，表示终端 $u_i$ 为非空闲终端； $U_{2}$ 表示非忙碌终端集合，即当 $u_i\in U_2$ 时，表示终端 $u_i$ 为非忙或空闲终端，即前文提到的微服务器或协作者，在一定条件下，可协助邻近终端进行 D2D 卸载。$F_i$ 表示终端 $u_{i}(u_{i}\in U_{1})$ 自身的计算能力，$F_{j}$ 表示边缘服务器和微服务器 $b_{j}(b_{j}\in B\cup U_{2})$ 的计算能力。




为了贴切地描述每个终端的孪生模型更新请求或任务，本文使用一种具有普适性的任务模型来描述终端的任务，即将终端 $u_{i}(u_{i}\in U_{1})$ 的任务表示为 $Q_i$，$Q_{i}=(s_{i},I_{i},O_{i},w_{i},\tau_{i})$ 为一个五元组。其中，==$s_i$ 表示终端 $u_{i}$ 的孪生模型索引==； $I_{i}$ 和 $O_i$ 分别表示卸载 $Q_{\mathrm{i}}$ 时上传和下载的数据大小（bit）； $w_{i}$ 表示执行单位上传数据所需要的 CPU cycles，即执行 $Q_{i}$ 所需要的总 CPU cycles 为 $W_{i}=w_{i}\times I_{i}$ ； $\tau_i$ 表示任务 $Q_i$ 的最大可容忍时延。==孪生模型的更新请求涉及数据的整合与同步、状态评估、模型校准与验证等操作==。

#### 三组决策变量

**（1）卸载决策变量 $\mathbf{a}$**

一个 $N_{1}\times(1+M+N_{2})$ 维的、由 $a_{ij}\in\{0,1\}$ 组成的二进制矩阵 $\mathbf{a}$，表示所有终端的卸载决策。$a_{ij}=1$ 表示 $u_{i}(u_{i}\in U_{1})$ 将任务卸载到边缘服务器 $b_j(b_j\in B)$ 或进行 D2D 通信将任务卸载到微服务器 $b_{j}(b_{j}\in U_{2})$ ；当 $b_j=0$，$a_{ij}=1$ 时，即 $a_{i0}=1$ 时表示 $u_i$ 未进行任务卸载，而是使用自身 CPU 执行任务； $a_{ij}=0$ 表示终端 $u_i$ 未将任务卸载到 $b_{j}(b_{j}\in\{B\cup U_{2}\})$。需要注意的是，本文将任务视为独立对象，在做出任务卸载决策时，任务只能作为整体进行卸载。

**（2）==孪生模型==部署决策变量 $\mathbf{b}$**

~~一个 $K\times M$ 维矩阵 $\mathbf{b}=\{b_{kj}\in\{0,1\}| S_{k}\in S,b_{j}\in B\}$，表示服务部署决策。$b_{kj}=1$ 表示 $b_{j}$ 部署服务 $S_k$，可为有此服务请求的终端提供服务响应； $b_{kj}=0$ 表示 $b_{j}$ 未部署服务 $S_k$。服务部署决策主要针对 MEC 服务器而言，在现实中，作为微服务器的终端不会仅为帮助邻近终端执行任务而特意花很大代价部署一个新服务，所以终端进行 D2D 卸载的前提是作为卸载目的节点的微服务器部署了终端所请求的服务。~~

==更正==：一个 $N_1 \times M$ 维矩阵 $\mathbf{b}=\{b_{ij}\in\{0,1\}| s_{i}\in S,b_{j}\in B\}$，表示孪生模型部署决策。$b_{ij}=1$ 表示 $b_{j}$ 部署孪生模型 $s_i$； $b_{ij}=0$ 表示 $b_{j}$ 未部署孪生模型 $S_i$。孪生模型部署决策主要针对 MEC 服务器而言，在现实中，作为微服务器的车辆终端不会仅为帮助邻近终端执行任务而特意花很大代价部署一个新模型。==所以车辆终端进行 D2D 卸载之后，需要将执行结果传输到孪生模型所在边缘服务器==。

**（3）资源分配决策变量 $\mathbf{c}$**

一个 $N_{1}\times(M+N_{2})$ 维矩阵 $\mathbf{c}=\{c_{ij}\in[0,1]|u_{i}\in U_{1},b_{j}\in\{B\cup U_{2}\}\}$，$c_{ij}$ 为 0 到 1 之间的连续变量，表示终端 $u_i$ 将任务卸载到 $b_{j}$ 时，$b_{j}$ 为 $u_i$ 所分配的计算资源占边缘服务器 $b_{j}$ 计算资源总容量的比例，即资源分配决策。资源分配决策与计算卸载决策紧密相关，即 $c_{ij}\neq0$ 的前提是 $a_{ij}\neq0$，即$c_{ij} \le z_{ij}$。


#### 1.1.2 ==信息年龄模型==

信息年龄（Age of Information，AoI）是衡量信息新鲜度的指标，即信息从产生到当前时刻的时间间隔。对于孪生模型的更新来说，模型的 AoI 是模型使用的信息的年龄，即有效数据从生产到当前时刻的时间间隔。为了更方便地对孪生模型 AoI 进行建模，本文设本地终端周期性地产生新的数据，用于孪生模型更新。

<div align="center" ><img src="https://fastly.jsdelivr.net/gh/wlchenGG/myPictureBed@main/images/fig-aoi.png" alt="fig-aoi" width="500" style="box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);border-radius:10px;"/><br><div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #777; font-size: 90%; padding: 1px;">图 1.3 信息年龄示意图</div></div>

为此，我们首先将时间划分为连续的时隙片段，即 $\tau = 1,2,\dots$，且设每个时隙片段的时长为 $\Delta$，如图 1.3 所示。在每个时隙 $\tau$ 内，假设部分终端 $u_i \in U_1$ 在时隙的开头产生数据更新请求，然后经过数据卸载处理，更新其孪生模型，共耗时 $\delta_{\tau}^{\text{pre}}$。从数据生成时刻开始，其 AoI 逐渐随时间增加而增大，AoI 最大值 $\Delta+\delta_{\tau}^{\text{pre}}$，此时新的数据完成模型更新，AoI减小，并开始下一个周期的更新。因此，对于时隙 $\tau$ 内的某时刻 $t_{\tau} \in [0, \Delta]$，孪生模型的 AoI 可表示为：
$$
  \begin{align}
    \Delta_{t_{\tau}} = \begin{cases}
      \Delta + t_{\tau}, & \text{if } t_{\tau} \leq \delta_{\tau}^{\text{pre}}, \\
      t_{\tau}, & \text{if } t_{\tau} > \delta_{\tau}^{\text{pre}}.
    \end{cases}
  \end{align}
$$
峰值AoI可以充分体现系统信息的新鲜度，因此，时隙 $\tau$ 内的孪生模型峰值 AoI 为：
$$
  \begin{align}
  \hat{\Delta}_{k} = \Delta + \delta_{\tau}^{\text{pre}}
  \end{align}
$$
可见，峰值 AoI 与数据更新请求处理的时延 $\delta_{\tau}^{\text{pre}}$ 相关。

#### 1.1.3 计算及通信模型

在边缘网络中，终端的感知时延主要由计算时延和通信时延共同决定。在车载边缘网络中，多个移动终端将同时共享边缘服务器的计算资源来处理它们的任务，但是，在面临任务请求量随终端数量增加而迅速增加的情况时，少规模的移动边缘节点无法保证为所有终端提供满意的服务体验质量。因此，网络运营商需要根据有限的网络资源做出有效的决策以协助部分终端执行任务。


在车载边缘计算网络中，用 $T_{ij}^{M}$ 表示终端 $u_i$ 的任务 $Q_{i}$ 在整个卸载过程中的计算时延，$c_{ij}$ 表示 $b_{i}$ 为 $Q_{i}$ 分配的计算资源比例，为未知变量，$T_{ii}^{M}$ 计算如下：

$$
\begin{align}
  T_{ij}^{M}=\frac{W_{i}}{c_{ij}F_{j}},\forall u_{i}\in U_{1},b_{j}\in\{B\cup U_{2}\}, \tag{1.1}
\end{align}
$$

其中，$W_{i}$ 表示终端 $u_{i}$ 完成任务 $Q_{i}$ 所需要的总 CPU cycles，即前文提到总 CPU cycles 计算方式为 $W_{i}=w_{i}\times I_{i}$。

若终端未进行任务 MEC 卸载或 D2D 卸载，则终端在自身计算能力满足任务最大可容忍时延的前提下选择本地执行，即由自身设备 CPU 提供计算能力执行。终端自身执行任务的优点之一是无传输时延，缺点是任务执行会受到自身设备计算能力的限制，时延要求可能得不到保证，同时具有一定的能量消耗。本文用 $T_{i}^{L}$ 表示终端未进行任务卸载时、在自身设备执行任务的计算时延，计算如公式（1.2）所示：

$$
\begin{align}
  T_i^L=\frac{W_i}{F_i},\forall u_i\in U_1. \tag{1.2}
\end{align}
$$

如前所述，$F_{i}$ 表示终端 $u_i$ 自身的可用计算资源。终端与边缘/微服务器之间的通信时延一般由数据传输时延组成。数据传输时延由传输的数据量和链路带宽共同决定。由于边缘服务器在专用的通信链路上操作和接收数据，因此每个终端在向边缘服务器传输数据时，可感知来自向同一边缘服务器发送数据的其它终端的干扰，即小区内干扰。

本文用 $R_{ij}^U$ 表示终端 $u_{i}$ 将任务卸载到 $b_{j}(b_{j}\in\{B\cup U_{2}\})$ 的上行数据传输速率，计算如公式（1.3）。其中，$W_{ij}$ 表示上行子信道带宽，$G_{ij}$ 和 $p_{ij}$ 分别表示 $u_i$ 与 $b_{j}(b_{j}\in B)$ 之间的信道增益和上行传输功率，$\sigma_{ij}^2$ 表示噪声功率，小区内干扰 $I_{ij}=\sum_{n\in N_{j},n\neq i}G_{nj}p_{nj}$，集合 $N_j=\{i\in U_1|a_{ij}=1\}$ 表示将任务卸载到 $b_{j}$ 的终端组成的集合。同理，$G_{ij}^{\prime}$ 和 $p_{ij}^{\prime}$ 分别表示终端与微服务器之间的信道增益和传输功率，$N_{0}$ 表示噪声功率。

$$
\begin{align}
  R_{ij}^{U}=
  \begin{cases}
    W_{ij}\log(1+\frac{G_{ij}p_{ij}}{\sigma_{ij}^{2}+I_{ij}}), &\forall b_{j}\in B \\ 
    W_{ij}\log(1+\frac{G_{ij}^{\prime}p_{ij}^{\prime}}{N_{0}+I_{ij}^{\prime}}), &\forall b_{j}\in U_{2}
  \end{cases} \forall u_{i}\in U_{1}. \tag{1.3}
\end{align}
$$

同理，$b_{j}(b_{j}\in\{B\cup U_{2}\})$ 将 $u_{i}(u_{i}\in U_{1})$ 的任务执行完成后，将任务结果数据回传到==孪生模型所在边缘服务器。==

在本文的孪生模型 AoI 优化中，除了孪生模型数据的 AoI，同时我们也考虑到孪生模型数据到用户的回传过程，优化整个过程中的 AoI。因此，孪生模型的更新结果也进一步回传到车辆终端$u_i$，$u_i$ 的下行数据传输速率 $R_{ji}^D$ 计算如下，对于 $b_j\in B$，$R_{ji}^{D}= W_{ji}\log ( 1+ G_{ji}p_{ji} / (\sigma _{ji}^{2}+ I_{ji}) )$，对于 $b_{j}\in U_{2}$，$R_{ji}^{D}=W_{ji}\log(1+G_{ji}^{\prime}p_{ji}^{\prime}/(N_{0}+I_{ji}^{\prime}))$。

因此，终端 $u_{i}$ 将任务卸载到 $b_{j}$ 并获得执行结果的所花费的总通信时延 $T_{ij}^{UD}$，即==上行、中间数据传输和下行传输时延之和==，计算如下：

原公式：$T_{ij}^{UD}=\frac{I_i}{R_{ij}^U}+\frac{O_i}{R_{ji}^D}$

==更正为：==

$$
\begin{align}
  T_{ij}^{UD}=\frac{I_i}{R_{ij}^U}+\sum_{e \in D_{j,j'}} \frac{I_i'}{R_e} + \sum_{e \in D_{j',j}} \frac{O_i}{R_e} + \frac{O_i}{R_{ji}^D}. \tag{1.4}
\end{align}
$$

==其中，$\sum_{e \in D_{j,j'}} \frac{I_i'}{R_e}$表示从协作计算的节点将计算结果传输到车辆孪生模型所在边缘服务器的时延和（注意：数据计算结果$I_i'=k I_i$，$k \le 1$,即结果数据与原数据大小成正比，但远小于原数据），$D_{j,j'}$表示两个节点间的迪杰斯特拉最短路径。孪生模型的更新结果 $O_i$ 需要先从模型所在服务器 $b_{j'}$ 回传到车辆终端 $u_i$ 的覆盖服务器，然后再传回车辆终端 $u_i$，即式 (1.4) 中后两项。==

==另外，如果是本地计算，在计算完成后也要将计算结果传输到车辆孪生模型所在边缘服务器，然后孪生模型的更新结果要进一步回传到车辆终端。因此，本地处理时的数据传输时延公式为==：
$$
\begin{align}
  T_{i}^{U}=\frac{I_i}{R_{ij}^U}+\sum_{e \in D_{j,j'}} \frac{I_i'}{R_e} + \sum_{e \in D_{j',j}} \frac{O_i}{R_e} + \frac{O_i}{R_{ji}^D}. \tag{1.4.1}
\end{align}
$$

如上所述，移动终端的任务可以使用自身设备 CPU 执行，但由于自身计算能力的限制，大多数终端选择将任务卸载到邻近的微服务器或边缘服务器，以在短时延内得至任务执行结果。目前，移动终端大多配备多个 CPU 核，边缘服务器更是配备多个 CPU 核，可并行处理多个任务。将计算时延 $T_{ij}^M$ 和通信时延 $T_{ij}^{UD}$ 相结合，本文将终端进行任务卸载过程的总响应时延表示为 $T_{ij}^E$，计算如下
$$
\begin{align}
  T_{ij}^E=T_{ij}^M+T_{ij}^{UD}+T_{ij}^q\:,  \tag{1.5}
\end{align}
$$
其中，$T_{ij}^q$ 表示移动终端 $u_{i}$ 将任务卸载到 $b_{i}$ 时，停留在 $b_{i}$ 等待任务执行的等待时延，该等待时延与任务量成正比。

==同理，对于本地计算，将计算时延 $T_{i}^{L}$ 和通信时延 $T_{i}^{U}$ 相加，总响应时延 $T_{i}^{D}$ 计算如下==：
$$
\begin{align}
  T_{i}^{D}=T_{i}^{U}+T_{i}^{L}.  \tag{1.5.1}
\end{align}
$$

综上，本文假设移动终端的任务要么进行端到边缘或端到端卸载，要么在本地使用自身 CPU 执行，则终端 $u_{i}$ 完成任务 $Q_{i}$ 的总时延 $T_{i}$ 表达式如

==注意这里本地（车辆）执行计算，也需要将计算结果传输到孪生模型所在边缘服务器，然后将数据回传到车辆==
$$
\begin{align}
  T_{i}=a_{i0}T_{i}^{D}+\sum_{b_{j}\in\{B\cup U_{2}\}}a_{ij}T_{ij}^{E},\forall u_{i}\in U_{1}.  \tag{1.6}
\end{align}
$$

此外，本文考虑了终端的移动性，终端 $u_{i}(u_{i}\in U_{1})$ 的移动性将影响其与 $b_{j}$ 的通信过程。通过调研了解到，目前关于轨迹预测已经有广泛的研究，尤其是在智能交通领域，本文假设在短期时间 $T^{p}$ 内可以准确估计移动终端的轨迹。本文为整个车载边缘网络构建一个虚拟坐标系，左下角为坐标原点。设边缘服务器 $b_{j}(b_{j}\in B)$, 的坐标为 $(x_j,y_j)$，且边缘服务器位置固定，不随时间而变化。设移动终端 $u_{i}(u_{i}\in U_{1})$ 在 $t$ 时刻的坐标为 $(x_{i}(t),y_{i}(t))$，定义0 时刻为终端产生服务请求的时刻，则移动终端 $u_{i}$ 在预测时间 $T^{p}$ 内的轨迹可定义为：
$$
\begin{align}
  T_{i}(t)=(x_{i}(t),y_{i}(t)),t\in[0,T^{p}].  \tag{1.7}
\end{align}
$$

边缘服务器 $b_j(b_j\in B)$ 的通信覆盖区域可表示为由无数个点组成的圆形区域，该圆形区域内的所有离散点构成一个连续的点集，记为 $\mathcal{R}_j$。可理解为，当 $u_i$ 在 $t(t\geq0)$ 时刻位于 $b_{j}$ 的覆盖区域内时，可表示为 $\mathcal{T}_{i}(t)\in \mathcal{R}_{j}$ ；当 $u_{i}$ 在 $t$ 时刻不在 $b_{i}$ 的覆盖区域内时，可表示为 $\mathcal{T}_i(t) \not\in \mathcal{R}_j$。

如前文所述，在密集多单元网络中，本文不考虑由于终端的移动性从而使终端在多个微基站之间进行切换执行任务的情况，在这种情况下，时延很可能得不到保证。因此，移动终端 $u_{i}$ 任务卸载的整个过程需要保证一直在 $b_{j}$ 的通信范围内，即覆盖范围区域内可表示为公式（1.8）：
$$
\begin{align}
  \mathcal{T}_{i}(t)\in\mathcal{R}_{j},\forall t\in[0,T_{ij}^{E}],u_{i}\in U_{1},b_{j}\in\Omega_{i},  \tag{1.8}
\end{align}
$$
其中，$\Omega_{i}$ 表示 $u_i$ 在 0 时刻的位置 $T_i(0)$ 属于哪些服务器通信范围内的服务器集合，$T_{ij}^{E}$ 表示终端 $u_i$ 将任务卸载到 $b_{j}$ 的总响应时延。

为更好地模拟终端的移动场景并方便计算，本文将终端在任务卸载过程中的空间约束，即公式（1.8），转换为一维的时间约束，如公式（1.9）
$$
\begin{align}
  T_{ij}^{E}\leq T_{ij}^{l},\forall u_{i}\in U_{1},b_{j}\in\Omega_{i},  \tag{1.9}
\end{align}
$$
其中，$T_{ij}^l$ 表示移动终端 $u_i$ 离开 $b_{j}$ 覆盖区域的时间，可根据移动终端的历史移动数据通过机器学习等方法来估计。

### 1.2 优化问题构建与转化

本文在车载边缘网络场景下根据网络中的动态条件，联合优化卸载决策、孪生模型部署决策和资源分配分配，其目的在于最小化终端 AoI。下面，详细介绍构建多维约束下最小化时延的计算卸载问题的过程以及问题的转化。

#### 1.2.1 多维约束下最小化时延的计算卸载问题

宏基站实时感知终端的位置和网络资源等信息，经过一定计算，做出高效的孪生模型部署决策 $\mathbf{b}$ 和终端计算卸载决策 $\mathbf{a}$，以及最优计算资源分配决策 $\mathbf{c}$，使终端获得高服务体验质量，并保证服务的 QoS。

本文的研究目的是帮助网络运营商决定在每台边缘服务器部署哪些服务，并制定任务卸载决策和资源分配决策，以最小化终端的孪生模型 AoI，同时提高资源利用率。对于每个 $\tau$，我们==优化 AoI 的目标==如公式（1.10）所示：

$$
\begin{align}
  \min_{\mathbf{a},\mathbf{b},\mathbf{c}} 
  \frac{\sum_{u_i \in U_1} \sum_{b_j \in \{B \cup U_2\}} a_{ij}  (\Delta + T_{ij}^{E})} 
{\sum_{u_i \in U_1} \sum_{b_j \in \{B \cup U_2\}} a_{ij}}
 \tag{1.10}
\end{align}
$$
其中，$T_{ij}^{E}$ 即为公式 (2) 中孪生模型更新前的数据处理时延 $\delta_{\tau}^{\text{pre}}$。



计算卸载决策 $\mathbf{a}$、孪生模型部署决策 $\mathbf{b}$ 和资源分配决策 $\mathbf{c}$ 三类决策变量，都需要满足网络中的一些固有条件，例如资源限制等约束，详细介绍如下。

首先，终端若卸载任务，如前所述，只能卸载到一台边缘服务器或微服务器；否则，终端使用自身 CPU 执行任务：
$$
\begin{align}
  \sum_{b_{j}\in\{B\cup U_{2}\}}a_{ij}+a_{i0}=1,\forall u_{i}\in U_{1}.  \tag{1.11}
\end{align}
$$

~~第二，若终端 $u_{i}$ 将任务卸载到边缘服务器 $b_{i}(b_{i}\in B)$，需要保证 $u_{i}$ 所请求的服务必须部署在 $b_{j}$ 中，若 $u_{i}$ 将任务卸载到微服务器 $b_{j}(b_{j}\in U_{2})$ 中，前提条件是微服务器 $b_{j}$ 部署了终端 $u_i$ 所请求的服务，于是孪生模型部署决策与终端计算卸载决策之间的依赖关系可表示为公式（1.12）：~~
$$
\begin{align}
  a_{ij}\leq b_{S_{i}j},\forall u_{i}\in U_{1},b_{j}\in\{B\cup U_{2}\},  \tag{1.12}
\end{align}
$$
==1.12约束不需要==



第三，边缘服务器 $b_{j}$ 中所部署孪生模型的总数据量不能超过 $b_{j}$ 的最大存储容量，即需要满足公式（1.13）：
$$
\begin{align}
  \sum_{S_{k}\in S}b_{ij}r_{i}\leq R_{j},\forall b_{j}\in B,  \tag{1.13}
\end{align}
$$
其中，$r_k$ 为服务 $S_k$ 的数据大小，$R_{j}$ 为边缘服务器 $b_{j}$ 的存储容量上限值。

第四，每个终端完成任务 $Q_{i}$ 的时延必须满足 $Q_{i}$ 的最大可容忍时延 $\tau_{i}$：
$$
\begin{align}
  T_{i}\leq\tau_{i},\forall u_{i}\in U_{1}.  \tag{1.14}
\end{align}
$$

第五，在车载边缘网络中，当 $u_{i}$ 卸载任务到 $b_{j}$ 时，需要保证移动终端的整个计算卸载过程在 $b_{i}$ 的通信覆盖区域内完成，即公式（1.9）等价于公式（1.15）：
$$
\begin{align}
  a_{ij}T_{ij}^{E}\leq a_{ij}T_{ij}^{l},\forall u_{i}\in U_{1},b_{j}\in\{B\cup U_{2}\},  \tag{1.15}
\end{align}
$$
其中，$T_{ij}^E$ 为 $u_i$ 卸载任务到 $b_{j}$ 执行的总响应时延。

第六，由于在时间 $T^{p}$ 后移动终端的轨迹无法预测，为保证终端的服务体验质量，终端 $u_{i}$ 在整个计算卸载过程中的时间消耗需保持在 $T^{p}$ 内：
$$
\begin{align}
  a_{ij}T_{ij}^{E}\leq T^{p},\forall u_{i}\in U_{1},b_{j}\in\{B\cup U_{2}\},  \tag{1.16}
\end{align}
$$
由于终端在短时间内的轨迹预测可达到较高的精度，本文假设 $T^{p}$ 在几秒范围内。

第七，服务器 $b_{i}$ 计算负载不能超过 $b_{i}$ 的计算资源上限：
$$
\begin{align}
  \sum_{u_i\in U_1}c_{ij}\leq1,\forall b_j\in\{B\cup U_2\}.  \tag{1.17}
\end{align}
$$

综上所述，目标函数（1.10）需要满足上述约束，从而制定孪生模型部署决策、计算卸载决策与资源分配决策，以最小化网络的总信息年龄。因此，本文将多维约束下==最小化平均 AoI==的计算卸载问题表示如下：
$$
\begin{align}
    &\min_{\mathbf{a},\mathbf{b},\mathbf{c}} 
  \frac{\sum_{u_i \in U_1} \sum_{b_j \in \{B \cup U_2\}} a_{ij}  (\Delta + T_{ij}^{E})} 
{\sum_{u_i \in U_1} \sum_{b_j \in \{B \cup U_2\}} a_{ij}}   \tag{1.18} \\
  &\text{s.t. Constraints:(3.11)-(3.17).} \notag
\end{align}
$$

#### 1.2.2 双层优化问题转化

多维约束下最小化 AoI 的计算卸载问题存在离散变量（任务卸载决策和孪生模型部署决策）和连续变量（资源分配决策），且存在非线性约束，为一个 MINLP 问题，同时也是一个 NP-hard 问题。根据观察，可推断出这三类决策变量存在一定的依赖关系：

~~（1）任务卸载决策依赖于孪生模型部署决策，做出终端卸载决策的前提是卸载目的节点已部署终端所请求的服务，即只有在孪生模型部署完成后，才能在有限的网络资源内做出有效的计算卸载决策：~~

（2）根据终端的任务卸载决策，MEC 服务器为卸载任务的终端进行计算资源再分配，从而得到最优计算资源分配决策：

（3）只有在计算资源再分配完成后，才能有效地评估计算卸载决策和孪生模型部署决策的性能。

如上所述，由于孪生模型部署、计算卸载和资源分配三类决策变量存在一定的依赖关系，本文将问题 $\mathcal{P}$ 转化为双层优化问题，如公式（1.19）所示，且有 $P1$ 等价于 $\mathcal{P}$。该双层优化问题是在保证下层优化问题最优性的前提下对上层优化问题进行处理。本文将孪生模型部署决策和计算卸载决策问题纳入上层优化，以==最小化终端平均 AoI== ，将计算资源分配决策问题归结为下层优化，目的在于提高服务 QoS 和减少终端计算时延，其原因是计算资源的分配与终端任务卸载过程中的计算时延 $T_{ij}^M$ 具有紧密的联系。

$$
\begin{align}
  &\mathcal{P}1: \quad \min_{\mathbf{a},\mathbf{b},\mathbf{c}} 
  \frac{\sum_{u_i \in U_1} \sum_{b_j \in \{B \cup U_2\}} a_{ij}  (\Delta + T_{ij}^{E})} 
{\sum_{u_i \in U_1} \sum_{b_j \in \{B \cup U_2\}} a_{ij}} \notag  \\
  &\text{s.t.} \quad \mathbf{c} \in \arg\min_{\mathbf{c}} \left\{ \sum_{u_{i}\in U_{1}}\sum_{b_{j}\in B} a_{ij}T_{ij}^{M} :(1.14)-(1.17) \right\}. \tag{1.19} \\
  & \qquad (1.11)-(1.13). \notag
\end{align}
$$

### 1.3 双层最小化卸载时延问题优化

为求解转化后的双层优化问题 $\mathcal{P}1$，以最小化网络的峰值 AoI，并在有限时间内得到高效的孪生模型部署和计算卸载决策、以及最优计算资源分配决策，本文提出一种启发式计算卸载与服务优化算法。本节主要对启发式计算卸载与服务优化算法总框架与执行流程进行介绍。

#### 1.3.1 优化方法设计

如上一节所述，原目标优化问题 $\mathcal{P}$ 已被转化为双层优化问题 $\mathcal{P}1$，但由于其问题的复杂度，依然难以求其最优解，于是，本文提出一种求解 $\mathcal{P}$l 的双层优化方法一一启发式计算卸载与服务优化算法（Heuristic Computation Offloading and Service Optimization Algorithm，HCASO）和基于时延梯度的计算资源分配算法（Delay Gradient-Basec Computing Resource Allocation Algorithm, DGRA），HCASO 算法总框架伪代码如算法 1.1 所示。

---
**算法1.1 (总体流程)：** 启发式计算卸载与模型放置算法
**Input：** 最大迭代次数 $g_{max}$
**Output：** 计算卸载、模型部署、资源分配决策 $\mathbf{a},\mathbf{b},\mathbf{c}$

1. 对车辆数据卸载节点剪枝，得到每个车辆数据的可行卸载节点集（边缘服务器/协作车辆） $\mathbf{DB}^0$（==算法 2.1==）
2. $g=0$
3. **`while`** $g<g_{max}$ **`do`**
4. ----构建初始计算卸载决策 $\mathbf{a}$（==算法 2.2==）
5. ----构建孪生模型部署决策 $\mathbf{b}$，并更新计算卸载决策 $\mathbf{a}$（==算法 2.3==）
6. ----评估本次解决方案
7. ----更新全局最优解
8. ----$g++$
9. **`end while`**
10. 执行基于时延梯度的计算资源分配算法，得到资源分配决策 $\mathbf{c}$（==算法 2.4==）
---


在 HCASO 算法中，首先，在初始化阶段，通过一定规则对每个终端（$u_i\in U_1$）的卸载节点进行剪枝，得到每个终端的可行卸载节点集合，如算法 1.1 第 1 行，即下文算法 2.1 所示。然后，在主循环中，上层优化在每一次迭代中实现，孪生模型部署决策嵌套在计算卸载决策中。具体来说，上层优化是使用带有排序优先级的策略来构建孪生模型部署决策和卸载决策，并进行交替优化，同时对孪生模型部署决策和卸载决策的性能进行评估，并根据评估结果决定是否更新全局最优解，如下文算法 2.2 和算法 2.3 所示，即算法 1.1 第 4 行-第 8 行，直到迭代到最大迭代次数 $g_{max}$，$g_{max}$ 根据实验仿真而得。最后，在下层优化中，根据已求得的卸载决策，如算法 1.1 第 10 行所示，执行基于时延梯度的计算资源分配方法以得到相应的计算资源最优分配决策，即执行算法 2.4 得到 $\mathbf{c}$。

在算法 1.1 主循环中得到孪生模型部署和计算卸载决策后，为确保本次迭代得到的孪生模型部署决策和卸载决策的有效性和高效性，以及为下层优化中的计算资源分配决策提供良好的支持，评估本次选代中由两类决策变量组成的解决方案的性能至关重要。

在 HCASO 算法评估过程中，为了使解决方案更好地贴近优化目标，首先最大化计算卸载的终端数量，然后最小化终端的 AoI，并将这两个函数作为算法 1.1 中评估解决方案的性能指标，评估函数表示如下：

**（1）评估函数 1：** $F_1(\mathbf{a},\mathbf{b})$，最大化车载边缘网络的响应率，即最大化卸载的终端数量，即 $F_{\mathrm{l} }( \mathbf{a} , \mathbf{b} ) = \sum _{u_{i}\in U_{1}}\sum _{b_{j}\in ( B\cup U_{2}) }a_{ij}$ ：

**（2）评估函数 2：** $F_2(\mathbf{a},\mathbf{b})$，最小化卸载任务终端的 AoI，也是问题 $\mathcal{P}$ 和 $\mathcal{P}$1 的优化目标，即 $F_{2}(\mathbf{a},\mathbf{b})=\frac{\sum_{u_i \in U_1} \sum_{b_j \in \{B \cup U_2\}} a_{ij}  (\Delta + T_{ij}^{E})} 
{\sum_{u_i \in U_1} \sum_{b_j \in \{B \cup U_2\}} a_{ij}}$

对于评估函数 $F_1(\mathbf{a},\mathbf{b})$ 和 $F_2(\mathbf{a},\mathbf{b})$，在 HCASO 算法评估过程中，首选 $F_{\mathrm{l}}$ 值大的解决方案，若与前一次迭代得到的解决方案具有相同的 $F_{1}$ 值，则首选 $F_{2}$ 值小的解决方案。

最后，分析启发式计算卸载与服务优化算法的时间复杂度。
在 HCASO 算法初始化阶段，首先检查每个终端的卸载目的节点的可行性，并为每个终端过滤掉不可行卸载目的节点，因此，基于计算卸载的可行卸载节点剪枝算法（算法 2.1）的时间复杂度为 $O(N_{1}(M+N_{2}))$。
在构建初始计算卸载决策阶段，使用快速排序法生成终端优先级，其时间复杂度为 $O(N_{1}\log(N_{1}))$，然后根据终端优先级为每个终端从各自可行的卸载目的节点集合中依概率选择一个可行卸载目的节点，其时间复杂度不超过 $O(N_{1}(M+N_{2}))$，于是，最小化时延卸载决策算法(算法 2.2)的时间复杂度为 $O(\operatorname*{max}\{N_{1}\log(N_{1}),N_{1}(M+N_{2})\})$。
在孪生模型部署决策构建和计算卸载决策更新阶段，根据算法 2.3 可计算出其平均时间复杂度为 $O(N_{1}(1+\log(N_{1}/M)))$。在下层优化阶段，需要计算给定卸载决策情况下计算资源的最优分配决策，算法 2.4 的时间复杂度则取决于所划分的计算资源原子块大小，假设平均每台边缘服务器将剩余可利用计算资源划分为 $n_a$ 块相同大小的原子块，则基于时延样度的计算资源分配算法（算法 2.4）的平均时间复杂度约为 $O(N_{\mathrm{l}}+Mn_{a})$。
综上所述，HCASO 算法的总时间复杂度约为 $O(N_{1}M+N_{1}N_{2}+Mn_{a}+\max\{N_{1}\log(N_{1}),N_{1}(M+N_{2})\})$。

#### 1.3.2 方法执行流程

如图 1.4 所示，终端 $u_i$ 的任务 $Q_{i}$ 被执行有三种情况：端到边缘卸载执行、端到端卸载执行、本地执行。

<div align="center" ><img src="https://cdn.jsdelivr.net/gh/wlchengg/PicBed@main/images_for_blogs/20241004220724.png" alt="20241004220724" width="300" style="box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);border-radius:10px;"/><br><div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #777; font-size: 90%; padding: 1px;">图 1.4 任务执行过程</div></div>


在车载边缘网络中，为联合优化计算卸载决策、孪生模型部署决策和资源分配决策，本文提出一种启发式计算卸载与服务优化算法，且 HCASO 算法总框架如前所述，结合算法总框架和终端的任务执行过程，HCASO 算法在车载边缘网络中的执行过程主要分为四步，介绍如下：

（1）在车载边缘网络架构下，所有移动终端向基站提交自己的信息，例如计算任务信息基站也时刻对终端位置等信息进行感知。这些信息在当前 LTE 网络或未来 5G 网络中可通过物理上行控制信道传输，且该信息仅占少量数据位，传输成本低；

（2）综合所得到的信息，执行 HCASO 算法，得到孪生模型部署决策和计算卸载决策，以决定每台边缘服务器的服务资源调配和终端的任务调度。可通过下行控制信道将这些指令发送到微基站和终端；

（3）部署边缘服务器的微基站接收到相关指令后，每台 MEC 服务器执行基于时延梯度的计算资源分配算法，与此同时，终端根据收到的指令执行卸载决策。当终端进行端到边缘卸载与 MEC 服务器通信时，MEC 服务器分配相应的计算资源执行终端任务：对于进行端到端卸载的终端，直接通过 D2D 通信进行任务卸载；

（4）边缘服务器或微服务器将终端所卸载的任务执行完成后，将任务执行结果传输到孪生模型所在的边缘服务器，并将模型更新结果通过下行链路传输到对应终端。




## 2. 详细方案

本章基于上章提出的启发式计算卸载与服务优化算法总框架，详细介绍该算法的具体内容。启发式计算卸载与服务优化算法主要包含基于计算卸载的可行卸载节点剪枝算法、计算卸载与孪生模型部署决策动态交替优化算法和基于时延梯度的计算资源分配算法。

### 2.1 基于计算卸载的可行卸载节点剪枝

根据计算卸载决策、孪生模型部署决策和资源分配决策三类决策变量存在的依赖关系，多维约束下最小化时延的计算卸载问题 $\mathcal{P}$ 被等价转化为双层优化问题 $\mathcal{P}1$，如公式（1.19）所示。上层优化问题为最少化平均响应时征的计算卸载与孪生模型部署决策问题，下层优优问题为最小化终端计算时延的计算资源分配问题。


对于每个终端而言，有 $M+N_2$ 种可能卸载决策，即总卸载决策数量为 $(M+N_{2})\times N_{1}$，由于终端只能将任务进行整体卸载，终端只能将任务卸载到一台边缘服务器或微服务器。为减少不可行的孪生模型部署决策和计算卸载决策，本章在 HCASO 算法初始阶段对每个终端的卸载目的节点进行剪枝，过滤掉不可行的卸载目的节点，从而降低 HCASO 算法时间复杂度。

下面给出判断卸载目的节点是否为可行卸载目的节点的条件。对于终端 $u_{i}(u_{i}\in U_{1})$ 而言，若 $b_{j}(b_{j}\in\{b\cup U_{2}\})$ 能满足 $u_i$ 的最小计算资源需求，且 $u_i$ 在 $b_{j}$ 的通信范围内，则节点 $b_{j}$ 为 $u_{i}$ 的一个可行卸载目的节点。



对于计算资源而言，由于移动终端存在 CPU 负载高、当前可利用计算资源有限等问题，且不同的卸载节点提供的计算资源也可能不同，如果某卸载节点不能满足移动终端任务对计算资源的最小需求，则该卸载节点不能作为该终端的可行卸载目的节点。由公式（1.14）-（1.16）可知，如果终端 $u_{i}(u_{i}\in U_{1})$ 将任务卸载到 $b_{j}(b_{j}\in\{B\cup U_{2}\})$，则 $u_i$ 整个卸载过程的响应时延 $T_{ij}^E$ 需要满足不等式（2.1）：

$$
\begin{align}
  T_{ij}^{E}\leq\min\{T_{ij}^{l},T^{p},\tau_{i}\}.  \tag{2.1}
\end{align}
$$

将不等式（2.1）与公式（1.5）结合，可推导得出不等式（2.2），即 $u_{i}$ 卸载任务到 $b_{j}$ 时，其执行任务的计算时间 $T_{ij}^M$ 需满足以下不等式：

$$
\begin{align}
  T_{ij}^M\leq\min\{T_{ij}^l,T^p,\tau_i\}-T_{ij}^{UD}-T_{ij}^q,  \tag{2.2}
\end{align}
$$

令 $T_{ij}^{1}=\min\{T_{ij}^{l},T^{p},\tau_{i}\}-T_{ij}^{UD}-T_{ij}^{q}$，则公式（2.2）等价于 $T_{ij}^M \leq T_{ij}^1$，进而，$u_i$ 卸载任务到 $b_{j}$ 的整个卸载过程的总响应时延需要满足：1）在任务最大可容忍时延 $\tau_i$ 内，2）在终端 $u_i$ 软迹预测时间 $T^{p}$ 内，3）在终端 $u_i$ 未离开所卸载目的节点的通信范围内。因此，$b_{j}$ 分配给 $u_i$ 的计算资源比例 $c_{ij}$ 需要满足的条件及比例下界 $c_{ij}^{min}$ 为

$$
\begin{align}
  c_{ij}\geq\frac{W_i}{T_{ij}^MF_j}\Rightarrow c_{ij}^{min}=\frac{W_i}{T_{ij}^1 F_j}.  \tag{2.3}
\end{align}
$$

综上所述，在 HCASO 算法初始化阶段剪枝步骤中，若 $b_{i}$ 为 $u_{i}$ 的可行卸载目的节点，则 $b_{j}$ 需要满足以下两个条件：
- (1) $u_i$ 在 $b_{j}$ 的通信范围内；
- (2) $c_{ij}^{min}\in[0,1]$ ；
- ~~(3）当 $b_j\in U_2$ 时，终端 $u_{i}$ 所请求的服务类型为 $b_{j}$ 已有服务。~~

为减少对不可行的孪生模型部署决策和计算卸载决策，在算法初始阶段对每个车辆数据的卸载目的节点进行剪枝，过滤掉不可行的卸载目的节点，从而降低算法复杂度。

可行卸载目的节点的条件：对于车辆 $u_i$ 而言，若 $b_j$ 能满足 $u_i$ 的最小计算资源需求 $c_{ij}^{\text{min}}$，且 $u_i$ 在 $b_j$ 的通信范围内（$j$ 的通信半径 $R_j$ 内），则节点 $b_j$ 为 $u_i$ 的一个可行卸载目的节点。

于是，基于计算卸载的可行卸载节点剪枝方法伪代码如算法 2.1 所示。

---
**算法2.1：** 基于计算卸载的可行卸载节点剪枝
**Input：** $\mathbf{R}$
**Output：** $\mathbf{DB}^0$
1. 初始化 $\mathbf{DB}^0$
2. 计算 $\mathbf{c}^{\text{min}}$，$\mathbf{distance}$
3. **`for`** $u_i \in U_b(t)$ **`do`**
4. ----**`for`** $b_j \in \{V \cup U_f(t)\}$ **`do`**
5. --------**`if`** $distance(i,j) \leq \mathcal{R}_j$ **`and`** $c_{ij}^{\text{min}} \le 1$ **`then`**
6. ------------$\mathcal{DB}^0_i \leftarrow \mathcal{DB}^0_i \cup \{b_j\}$
7. --------**`end if`**
8. ----**`end for`**
9. **`end for`**
---

在算法 2.1 中，本文将终端 $u_{i}$ 的可行卸载目的节点集，即满足上述条件的边缘/微服务器 $b_{j}$ 的节点集，记作集合 $\mathcal{DB}_i^0$，$\mathcal{DB}_i^0\in\mathbf{DB}^0$；$c_{ij}^{min}(c_{ij}^{min}\in\mathbf{c}^{min})$ 表示终端 $u_{i}$ 将任务卸载到 $b_{j}$ 时，执行任务所需要的最小计算资源值； $\mathcal{R}_{j}(\mathcal{R}_{j}\in\mathbf{R})$ 表示 $b_{j}$ 的最大可通信范围半径，即覆盖区域半径； $distance(i,j)$ 表示 $u_i$ 与 $b_{j}$ 在 $t=0$ 时刻的距离，且 $distance(i,j)\in \mathbf{distance}$

### 2.2 计算卸载和孪生模型部署决策动态交替优化

对于双层优化问题 $\mathcal{P}$l，上层优化的目标是优化孪生模型部署决策和终端计算卸载决策，以最小化终端的平均响应时延。从启发式计算卸载与服务优化算法总框架可知，基于计算卸载的可行卸载节点剪枝算法执行完成后，需要对上层最小化响应时延优化问题进行求解。上层优化方案主要由三部分组成，分别是构建初始计算卸载决策（即最小时延卸载决策算法）、构建孪生模型部署决策、更新计算卸载决策，即计算卸载决策和孪生模型部署决策两类决策变量实现交替优化。

#### 2.2.1 最小时延卸载决策算法

在未得到孪生模型部署决策之前，先构建初始计算卸载决策，得到所有终端的初始卸载目的节点，为此，本文提出一种最小时延卸载决策算法。对于每个终端 $u_{i}(u_{i}\in U_{1})$，从其可行卸载目的节点集 $\mathcal{DB}_i^0$ 中按照一定规则选择一个卸载目的节点，从而获得初始卸载决策。若 $\mathcal{DB}_i^0$ 为空集，如前文所述，则终端 $u_{i}$ 不进行任务卸载，选择在本地使用自身设备 CPU 执行任务。下面详细介绍如何构建初始计算卸载决策。

首先，在终端选择初始计算卸载目的节点之前，先对所有终端生成终端优先级，由于大多数工作中终端优先级是随机的，这样做不利于构建可行的计算卸载决策。因此，本文采取这样一种策略来定义终端优先级一一以终端 $u_{i}$ 的可行卸载目的节点集 $\mathcal{DB}_i^0$ 的元素数量多少来决定优先级高低，即按照终端的可行卸载目的节点集的元素数量对所有终端进行排序，遵从可行卸载目的节点较少的终端具有更高优先级的规则。此优先级生成规则的优势在于，可以尽可能多地让更多的终端有可选择的可行卸载目的节点。

终端优先级生成示例如图 2.1 所示，假设有三个终端 $u_1,u_2,u_3$，$u_1$ 的可行卸载目的节点有 $\{b_1,b_2,b_3\}$，$u_2$ 的可行卸载目的节点有 $\{b_2,b_3\}$，$u_3$ 的可行卸载目的节点有 $\{b_3\}$。为了简单证明该优先级生成规则的有效性，假设 $b_1,b_2,b_3$ 各自只能响应单个终端的服务请求，首先由终端 $u_3$ 选择卸载目的节点 $b_3$，然后 $u_2$ 再选择卸载目的节点 $b_{2}$，最后 $u_1$ 选择卸载目的节点 $b_1$。只有当 $u_{3}$ 的优先级高于 $u_{2}$，$u_2$ 的优先级高于 $u_1$，这样才能使三个终端中每个终端都有可选择的可行卸载目的节点。同理，该终端优先级生成规则也适应于多密集单元移动边缘计算场景。

<div align="center" ><img src="https://cdn.jsdelivr.net/gh/wlchengg/PicBed@main/images_for_blogs/20241004222734.png" alt="20241004222734" width="400" style="box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);border-radius:10px;"/><br><div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #777; font-size: 90%; padding: 1px;">图 2.1 终端优先级生成示意图</div></div>

然后，计算每个终端 $u_{i}$ 选择 $\mathcal{DB}_i^0$ 中每个可行卸载目的节点的概率，若 $b_{j}\in\mathcal{DB}_{i}^{0}$，则 $u_{i}$ 选择将任务卸载到 $b_{j}$ 的概率 $p_{ij}$ 计算如公式（2.4）所示：

$$
\begin{align}
  p_{ij}=\frac{\xi_{ij}}{\sum_{b_{j}\in\mathcal{DB}_{i}^{0}}\xi_{ij}},\forall b_{j}\in\mathcal{DB}_{i}^{0},u_{i}\in U_{1},  \tag{2.4}
\end{align}
$$

其中，$\xi$ 表示与时延相关的启发式信息，即终端卸载到某可行卸载节点的时间消耗越少其终端选择该节点的概率也越大，启发式信息定义为 $\xi_{ij}=1/T_{ij}^E$，如前所述，$T_{ij}^E$ 表示终端 $u_{i}$ 将任务卸载到 $b_{j}$ 执行的总响应时延，包括上行时延、计算时延、排队等待时延和下行时延。注意，这里计算时延按照终端的最小计算资源需求 $c_{ij}^{min}$ 来计算，即 $b_{j}$ 为 $u_i$ 分配的计算资源的下界，后面会进行计算资源再分配。简言之，在得到最终卸载决策后，终端可能会被分配更多的计算资源。

最后，在得到终端优先级和终端选择各可行卸载目的节点概率后，即可做出初始卸载决策。本文采用依概率决策，以此得到初始卸载决策变量 $\mathbf{a}$。终端 $u_{i}$ 选择初始卸载目的节点 $b_{i}$ 主要通过以下规则，如公式（2.5）所示：

$$
\begin{align}
  j=\begin{cases}\arg\max_{b_{j^\prime}\in\mathcal{DB}_i^0}\xi_{ij^{\prime}},&\text{if}\:q\le q_0\\[2ex]J,&\text{otherwise}\end{cases},  \tag{2.5}
\end{align}
$$

其中，本文将 $q$ 设置为[0,1]中均匀分布的随机数，$q_0$ 为固定参数，$J$ 为根据 $p_{ij}$ 使用轮盘赌选择（RouletteWheelSelection，RWS）方法，从可行卸载目的节点集 $\mathcal{DB}_i^0$ 中选择的一个可行卸载目的节点。RWS 方法又称为按比例选择方法，其每个节点 $b_{j}(b_{j}\in\mathcal{DB}_{i}^{0})$ 被终端 $u_{i}$ 选中的概率与 $\xi_{ij}$ 成正比。由公式（2.5）可知，终端 $u_{i}$ 有一定的概率选择最大值 $\xi_{ij}(b_{j}\in\mathcal{DB}_{i}^{0}$, ）对应的节点 $b_{j}$，即选择最大 $p_{ij}$ 值对应的 $b_{j}$。初始计算卸载决策构建，即最小时延卸载决策算法伪代码如算法 2.2 所示。

---
**算法 2.2：** 初始计算卸载决策构建
**Input：** $\mathbf{F}$，$\mathbf{c}^{\text{min}}$，$\mathbf{F}^r$
**Output：** 计算卸载决策 $\mathbf{a}$
1. 调用==算法 2.1== 得到 $\mathbf{DB}^0$
2. 对所有车辆 $u_i \in U_1$，将其 $\mathcal{DB}^0_i$ 进行排序，生成终端优先级 $\mathcal{U}_1^p$
3. **`for`** $u_i \in \mathcal{U}_1^p$ **`do`**
4. ----**`if`** $\mathcal{DB}^0_i \neq \empty$ **`then`**
5. --------根据公式 ==(2.4)== 计算所有 $b_j \in \mathcal{DB}^0_i$ 的概率 $p_{ij}$
6. --------**`if`** $q \le q_0$ **`then`**
7. ------------$b_j \leftarrow \arg\max_{b_{j^\prime} \in \mathcal{DB}^0_i} \xi_{ij^\prime}$
8. ------------$a_{ij} = 1$
9. --------**`else`**
10. ------------根据公式 ==(2.5)== 在 $\mathcal{DB}^0_i$ 中选择 $b_j$
11. ------------$a_{ij} = 1$
12. --------**`end if`**
13. --------更新 $F_j^r \leftarrow F_j^r - c^{\text{min}}_{ij}$
14. ----**`elseif`** $\mathcal{DB}^0_i = \empty$ **`then`**
15. --------$a_{i0} = 1$
16. ----**`end if`**
17. **`end for`**
---

##### 2.2.2 孪生模型部署策略（==这个算法需要修改的多一点==）

根据上一节初始卸载决策构建步骤中得到的初始卸载决策 $\mathbf{a}$，可获取每个终端的拟卸载目的节点，从而也可得到每个节点 $b_{j}$ 拟协助的终端的集合，即为哪些终端执行卸载的任务。本文将 $b_{j}$ 拟协助的终端集合记为 $\mathcal{DU}_j^0$，${\mathcal{D}}U_{j}^{0}\in\mathbf{D}\mathbf{U}^{0}$，再根据 $\mathcal{DU}_j^0$ 中终端的==位置==~~服务请求类型~~得到边缘服务器 $b_{j}(b_{j}\in B)$ 的初始部署集 $\mathcal{S}_j^0$，$\mathcal{S}_j^0\in\mathbf{S}^0$，$\mathcal{S}_{j}^{0}$ 中的每个元素由孪生模型-==时延需求==~~服务类型-数量~~对组成，即 < ==孪生模型id，模型更新的时延需求==>，最后根据网络存储资源限制做出孪生模型部署决策。下面详细介绍构建孪生模型部署决策的过程。

首先，根据初始计算卸载决策 $\mathbf{a}$，得到每台边缘服务器 $b_j(b_j\in B_j)$ 的拟协助终端集 $\mathcal{DU}_j^0$，再根据 $\mathcal{DU}_j^0$ 中终端的==位置==得到初始孪生模型部署集 $\mathcal{S}_j^0=\{<s_{i},\tau_{i}>|s_{i}\in S\}$，其中 $\tau_{i}$ 记录孪生模型 $s_{k}$ ==更新时延需求。==~~被 $\mathcal{DU}_{i}^{0}$ 集合里的终端所请求的总次数~~。

然后，根据一定规则得到孪生模型部署优先级。对 $\mathcal{S}_{j}^{0}$ 中的元素按照==更新的时延需求$\tau_{i}$== ~~服务被请求总次数 $count_k$~~大小生成孪生模型部署优先级 $\mathcal{S}_j^*$，即==孪生模型更新越紧急的请求==~~被请求越多的服务~~其部署优先级越高。

最后，根据 $b_{j}$ 的存储资源大小限制得到孪生模型部署决策 $\mathbf{b}$。由于边缘服务器 $b_{j}$ 的存储资源有限，若通信范围内终端的服务请求类型呈多样化，则不一定能部署 $\mathcal{S}_{j}^{*}$ 中所有的服务。因此，在 $b_{j}$ 的存储资源限制内，本文按照服务优先级 $\mathcal{S}_{i}^{*}$ 依次做出孪生模型 $S_{k}(S_{k}\in\mathcal{S}_{j}^{*})$ 的部署决策 $b_{kj}$ 并更新存储资源占用。

~~本文考虑了服务实体与微服务实体的关系，一个服务实体由多个微务实体编排而成。因此，在做出孪生模型部署决策，更新存储资源占用时，需先确定编排为服务 $S_{k}=\{m_{k1},\cdots,m_{kn}\}$ 的微服务是否已部署。对于已部署的微服务，则可避免重复做出微孪生模型部署决策，即服务 $S_{k}$ 的存储资源占用只考虑该服务中未部署的微服务所占用的存储资源，从而节省存储资源以提高存储资源利用率。~~

#### 2.2.3 计算卸载决策优化

得到初始计算卸载决策 $\mathbf{a}$ 和孪生模型部署决策 $\mathbf{b}$ 之后，需要再次更新卸载决策，以确保所有终端的卸载决策的合理性。

首先，按照初始计算卸载决策构建过程中生成的终端优先级，轮询终端 $u_{i}(u_{i}\in U_{1})$ 的卸载决策是否可行，即是否满足 $b_{j}$ 的最大计算资源限制、==孪生模型更新时延需求==~~终端所请求的服务是否已在 $b_{j}$ 部署~~这两个条件。如果满足这两个条件，则 $u_i$ 的卸载决策 $a_{ij}=1$ 为可行决策，表示 $u_i$ 的初始计算卸载决策即为本次选代的最终计算卸载决策；对于存在 $a_{ij}=1$，但 $b_{j}$ 不满足终端 $u_{i}$ ==时延需求==~~所请求服务~~或 $b_{j}$ 计算资源不足的情况，则表示终端 $u_i$ 的卸载决策不合理，需要重新为这些终端制定卸载决策，将这些终端所组成的集合记为 $\mathcal{U}^{\prime}$。

然后，更新终端 $u_{i}(u_{i}\in\mathcal{U}^{\prime})$ 的可行卸载目的节点集 $\mathcal{DB}_i^0$，将更新完成的可行卸载目的节点集记为 $\mathcal{D}B_i^1$。遍历 $\mathcal{DB}_i^1$，将其中不满足==时延需求==~~卸载决策与孪生模型部署决策依赖关系，即不满足约束（1.12）~~和最大计算资源限制的节点进行再剪枝。这一步与基于计算卸载的可行卸载节点剪枝不同之处在于此步将约束（1.12）纳入其中。

最后，按照类似初始计算卸载决策构建中的步骤，为终端 $u_{i}(u_{i}\in\mathcal{U}^{\prime})$ 进行计算卸载决策更新。第一步依然是对终端 $u_{i}(u_{i}\in\mathcal{U}^{\prime})$ 生成终端优先级，然后计算终端 $u_{i}$ 选择卸载到节点 $b_j\in\mathcal{DB}_i^1$ 的概率 $p_{ij}$，最后根据公式（2.5）为终端 $u_{i}$ 重新选择卸载目的节点，从而达到在网络资源限制内为所有终端做出高效卸载决策。 

- 首先，按照初始计算卸载决策构建过程中生成的车辆数据优先级，轮询车辆 $u_i$ 的卸载决策是否可行，即是否满足 $v_j$ 的最大计算资源限制。如果满足这个条件，则 $u_i$ 的卸载决策 $x_{ij}=1$ 为可行决策，表示 $u_i$ 的初始计算卸载决策即为本次迭代的最终计算卸载决策；如果存在 $x_{ij}=1$，但前述条件不能满足时，需重新为这些车辆数据制定卸载决策，将这些车辆所组成的集合记为 $\mathbf{U}^\prime$。

- 然后，更新车辆 $u_i$ 的可行卸载目的节点集 $\mathbf{DB}_i^0$ ，将更新完成的可行卸载目的节点集记为 $\mathbf{DB}_i^1$。遍历 $\mathbf{DB}_i^1$，将其中不满足卸载决策，即不满足最大计算资源限制的节点进行再剪枝。

- 最后，按照类似初始计算卸载决策构建中的步骤，为车辆 $u_i$ 进行计算卸载决策更新。第一步依然是对车辆 $u_i$ 生成车辆数据优先级，然后计算车辆 $u_i$ 选择卸载到节点 $v_j$ 的概率 $p_{ij}$，最后根据公式（4.5）为车辆 $u_i$ 重新选择卸载目的节点，从而达到在网络资源限制内为所有车辆数据做出高效卸载决策。

在构建初始计算卸载决策后，孪生模型部署决策构建和计算卸载决策更新伪代码如算法 2.3 所示，其中 $R_{j}^{r}$ 表示 $b_{j}$ 剩余可利用的存储资源大小。

---
**算法 2.3：** 孪生模型部署决策构建和计算卸载决策更新
**Input：** $\mathbf{R}$，$\mathbf{R}^r$，$\mathbf{b}$
**Output：** 计算卸载、模型部署决策 $\mathbf{a},\mathbf{b}$

1. 调用==算法 2.2== 得到 $\mathbf{a}$
2. 根据 $\mathbf{a}$，得到每个 $b_j$ 拟协助的终端集合 $\mathcal{DU}_j^0 \in \mathbf{DU}^0$
3. **`for`** $b_j \in B$ **`do`**
4. ----**`for`** $u_i \in \mathcal{DU}_j^0$ **`do`**
5. --------更新 $\mathcal{S}^0_j$
6. ----**`end for`**
7. ----根据==孪生模型更新请求的时延==需求对 $\mathcal{S}^0_j$ 进行升序排序，生成服务优先级 $\mathcal{S}_j^*$
8. ----**`for`** $s_k \in \mathcal{S}_j^*$ **`do`**
9.  --------**`if`** $R^r_j - r_k > 0$ **`then`**
10. ------------$R^r_j = R^r_j - r_k$
11. ------------$b_{kj} = 1$
12. --------**`end if`**
13. ----**`end for`**
14. **`end for`**
15. **`for`** $b_j \in B$ **`do`**
16. ----判断 $\mathcal{DU}_j^0$ 中终端卸载决策的有效性，有效则更新 $F_j$，无效则将终端放入集合 $\mathcal{U}^\prime$
17. **`end for`**
18. 根据算法 2.1 得到的可行卸载节点集，并联合约束（1.17），更新 $\mathcal{U}^\prime$ 中终端的可行卸载节点集 $\mathbf{DB}^1$
19. 在不违背约束 1.17 的条件下，根据算法 （2.2） 和公式（2.5）更新 $\mathcal{U}^\prime$ 中终端的卸载决策
---

### 2.3 基于时延梯度的计算资源分配算法

在双层优化问题 $\mathcal{P}$l 中，通过求解上层最小化响应时延优化问题得到卸载决策和孪生模型部署决策后，需要对下层优化问题进行求解。下层优化问题是最小化终端计算时延的计算资源分配问题，本节通过研究时延与计算资源之间存在的函数关系，提出一种基于时延梯度的计算资源分配算法，下面详细介绍该算法。

#### 2.3.1 时延函数分析

下层优化的目标是在求得的孪生模型部署决策 $\mathbf{b}$ 和卸载决策 $\mathbf{a}$ 下，通过优化计算资源分配决策 $\mathbf{c}$，最小化所有进行卸载的移动终端的计算时延，如公式（2.6）所示，从而达到原优化问题平均响应时延最小化。本节所提出的资源分配算法主要针对卸载到边缘服务器的终端，因为微服务器空闲计算资源远小于边缘服务器，其微服务器所服务的终端数量也远小于边缘服务器所服务的终端数量。

$$
\begin{align}
  &\min_{\mathbf{c}}\sum_{u_{i}\in U_{1}}\sum_{b_{j}\in B}a_{ij}T_{ij}^{M}  \tag{2.6} \\
  &\text{s.t. (3.14)-(3.17)} \notag
\end{align}
$$

将公式（2.6）与公式（1.1）相结合可知，计算资源与计算时延存在严格的单调递减关系。假设计算时延与计算资源函数为 $h_{ij}(c)$，则 $h_{ij}(c)=T_{ij}^{M}=W_{i}/c_{ij}F_{j}$，则函数 $h_{ij}(c)$ 的梯度如公式（2.7）所示：

$$
\begin{align}
  \frac{\partial h_{ij}}{\partial c_{ij}}=-\frac{W_{i}}{c_{ij}^{2}F_{j}}\:.  \tag{2.7}
\end{align}
$$

如图 2.2 计算资源-时延函数示例图所示，横轴 $c$ 表示计算资源，纵轴 $h$ 表示计算时延，$\Delta c$ 表示边缘服务器的一块固定大小的计算资源，$\Delta h$ 则表示边缘服务器再给终端分配 $\Delta c$ 大小的计算资源时所对应计算时延的变化。从图 2.2 可知，$h$ 与 $c$ 如同计算时延与计算资源成反比关系，当 $\Delta c$l$=\Delta c2=\Delta c$ 时，有 $\Delta h$l>$\Delta h2$，即有 $(\Delta h1/\Delta c$l$)>(\Delta h2/\Delta c2)$ 成立，其等价于梯度绝对值越大，因变量变化越快。

<div align="center" ><img src="https://cdn.jsdelivr.net/gh/wlchengg/PicBed@main/images_for_blogs/20241004230441.png" alt="20241004230441" width="400px" style="box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);border-radius:10px;"/><br><div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #777; font-size: 90%; padding: 1px;">图 2.2 计算资源-时延函数示例图</div></div>

同理，对应于计算时延与计算资源之间的函数关系，当边缘服务器的可用计算资源有限时，可将可用计算资源划分为多个相同大小的计算资源块。借助函数梯度影响因变量这一思想，当边缘服务器为多个终端任务再分配计算资源时，为提高终端整体的服务体验质量，可优先将计算资源分配给时延函数 $h_{ij}(c)$ 梯度小的终端，即优先将计算资源分配给 $h_{ij}(c)$ 梯度绝对值大的终端。


#### 2.3.2 资源分配算法设计

为最小化所有计算卸载终端的计算时延之和，分配给每个终端的计算资源应该尽可能多，但由于终端数量的急剧增长，边缘服务器的计算资源难以满足所有终端的任务。因此，本文根据公式（2.3）计算出每个终端任务的最小计算资源需求 $c_{ij}^{min}$，执行计算卸载决策与孪生模型部署决策交替优化算法，得到终端的卸载决策 $\mathbf{a}$。

在为每个终端分配最小计算资源需求 $c_{ij}^{min}$ 后，判断边缘服务器 $b_j(b_j\in B)$ 是否还存在可利用的计算资源 $F_j^r$，如果存在 $F_j^r\neq0$，则根据计算时延与计算资源存在的函数关系，将 $b_{j}$ 剩余且未利用的计算资源 $F_{j}^{r}$ 划分为多个大小相同的计算资源原子块。本文将原子块大小的计算资源表示为 $ato\_piece$，并将原子块 $ato\_piece$ 分配给 $b_{j}$ 协助执行任务的终端。当原子块足够小时，最终的资源分配可充分接近最优的结果。下面详细介绍基于时延梯度的计算资源分配算法。

首先，根据孪生模型部署决策构建和计算卸载决策更新步骤获得 $\mathbf{a}$ 之后，可得到 $b_j(b_j\in B)$ 所负责的终端集 $\mathcal {DU}_j=\{u_i\mid a_{ij}=1,u_i\in U_1\}\in\mathbf{DU}$，同时通过计算可以得到每个终端 $u_{i}(u_{i}\in U_{1})$ 所需的最小计算资源需求 $c_{ij}^{min}$。

然后，针对集合 $DU_j$ 里的终端根据基于时延梯度的规则生成终端优先级，基于时延梯度的规则指按照计算时延与计算资源函数 $h_{ij}$ 梯度的大小生成终端优先级。如前所述，梯度越小，则计算资源原子块大小资源发挥的效果越大、终端计算时延变化越快，即梯度越小的终端，且分配到原子块大小计算资源的优先级越高。

最后，根据前一步所得终端优先级来分配计算资源原子块 $ato\_piece$，每当分配原子块大小的计算资源 $ato\_piece$ 时，便更新对应边缘服务器 $b_{j}$ 的剩余可利用计算资源 $F_{j}^{r}$、资源分配决策 $c_{ij}$ 、梯度 $h_{ij}^{\prime}$，直到 $F_{j}^{r}$ 小于 $F_j^{min}$，$F_j^{min}$ 表示 $b_{i}$ 所预留计算资源的上界值，预留计算资源主要用来处理来自非终端卸载的任务。综上所述，基于时延梯度的计算资源分配算法伪代码如算法 2.4 所示。

---
**算法 2.4：** 基于时延梯度的计算资源分配算法
**Input：** $\mathbf{a}$，$\mathbf{F}$，$\mathbf{DU}$，$ato\_piece$
**Output：** 计算资源分配决策 $\mathbf{c}$
1. 根据 $\mathbf{a}$，得到卸载终端的最小计算资源需求 $\mathbf{c}^{\text{min}}$，$\mathbf{c} \leftarrow \mathbf{c}^{\text{min}}$
2. 根据 $\mathbf{a}$，得到每个 $b_j$ 所负责的终端集合 $\mathcal{DU}_j$
3. **`for`** $b_j \in B$ **`do`**
4. ----$F_j^r = F_j - \sum_{u_i \in \mathcal{DU}_j} c^{\text{min}}_{ij} F_j$
5. ----计算 $\mathcal{DU}_j$ 中所有终端卸载的时延梯度 $\mathbf{h}_j^{\prime}=\{h_{ij}^{\prime} | u_i \in \mathcal{DU}_j\}$，$h_{ij}^{\prime}=-W_i / (c_{ij}^2 F_j)$
6. ----**`while`** $F_j^r - ato\_piece > F_j^{\text{min}}$ **`do`**
7. --------$u_i \leftarrow \arg\min_{u_i \in \mathcal{DU}_j} h_{ij}^{\prime}$
8. --------$c_{ij} = c_{ij} + ato\_piece/F_j$
9. --------更新梯度 $h_{ij}^{\prime}$，$F_j^r = F_j^r - ato\_piece$
10. ----**`end while`**
11. **`end for`**
---




## 3. 实验仿真与性能评价

本章主要对启发式计算卸载与服务优化算法和基于时延梯度的计算资源分配算法进行实验仿真，并研究终端数量、MEC 服务器计算能力和存储能力、以及终端任务卸载数据量和任务计算需求等对算法的影响，从而对算法进行多维性能指标的评价。

### 3.1 实验设置

本节主要对实验仿真环境进行介绍，以及对实验中的变量设置相应值，包括移动终端初始位置坐标和边缘服务器位置坐标设置、边缘服务器多维资源参数设置、服务多维参数等网络参数设置，并介绍性能评价的多维评估指标，例如平均响应时延、卸载终端数量等。

#### 3.1.2 实验参数

为验证本文所提出的启发式计算卸载与服务优化算法性能，首先对仿真实验的参数进行设置与说明，实验参数主要包括：与边缘服务器和终端相关的参数，与网络中的服务、终端所请求服务的资源需求相关的参数，以及网络中的其它参数。

（1）边缘服务器和终端相关参数

考虑到单宏基站多微基站的网络场景，本文将单宏基站下的微基站数量设置为 $M=9$，微基站的通信半径 $\mathcal{R}=150~m$，位置固定，且对于每一个微基站 $b_j(b_j\in B)$，设置基准存储容量 $R_{j}= 100~GB$ 、计算容量 $F_{j}= 20~GHz$。注意，这些值在评估过程中可能随着实验而改变，若无特殊说明则以此为基准。微基站有规律地部署在 $500~m \times 500~m$ 区域内的网络方格上，即前文所提到的虚拟坐标系，左下角为坐标原点，宏基站位于区域中心。

本文设置基准移动终端数量 $N=500$，在以后的实验中，具体的终端数量在实验中若无特殊说明则以此为基准。所有移动终端在初始时刻均匀分布在宏基站和微基站的覆盖范围内，并保证每个终端至少在一个微基站的覆盖区域内，即所有终端都可以至少与一个基站直接进行通信。为更好地模拟现实场景，本文定义参数 $\rho(\rho\in[0,1])$，表示终端产生服务请求的概率，即终端有 $\rho$ 的概率产生服务请求，等价于约有 $N\times\rho$ 的终端处于有服务请求且非空闲状态，约有 $N\times(1-\rho)$ 的终端处于无服务请求且非忙碌状态，若无特殊说明，设置 $\rho$ 基准值为 0.9。边缘服务器和移动终端初始位置和网络区域如图 3.1 所示，其中，红色三角形表示微基站（MEC 服务器）的位置坐标，蓝色和红色圆点分别表示产生和未产生服务请求的移动终端。

<div align="center" ><img src="https://cdn.jsdelivr.net/gh/wlchengg/PicBed@main/images_for_blogs/20241004231847.png" alt="20241004231847" width="400" style="box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);border-radius:10px;"/><br><div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #777; font-size: 90%; padding: 1px;">图 3.1 网络区域坐标图</div></div>

（2）==孪生模型参数==~~服务多维参数~~

==孪生模型：存储资源[1,10]GB，计算资源[0.3,2]GHz==

~~在车载边缘网络中，如前文所述，本文考虑了服务实体与微服务实体的关系，一个服务实体可以由多个微服务实体编排而成。在实验仿真中，本文将微服务种类数量设置为 $T=400$，服务种类数量设置为 $K=100$，即这 $T$ 种不同的微服务最多可组成 $K$ 种不同类型的服务，大部分移动终端从拥有 $K$ 种不同服务的服务库中请求一个延迟敏感或计算密集型服务。服务流行度遵循形状参数为 0.8 的 Zipf 分布，这是视频流等几种服务的常见假设。$K$ 种不同服务的资源需求随机映射到四种大类服务，分别是视频流（VideoStream，VS），人脸识别（FaceRecognition.FR），压缩（Gzip）和增强现实（AugmentedReality,AR），各类服务的具体参数设置如表 3.2 所示。~~

~~表 3.2 不同服务类型资源需求~~

| 服务粗粒度分类  | 存储资源(GB) | 计算资源(GHz) |
|----------------|----------|-------------|
| 视频流(VS)     | [1,10]   | [0.05,0.1]  |
| 人脸识别 (FR)  | [2,10] | [0.375,2]   |
| 压缩(Gzip)     | 0.02     | [0.04,0.32] |
| 增强现实 (AR)  | [2,12]   | [0.375,2]   |

~~视频流服务需要巨大的存储空间 $(1~GB-10~GB)$，但该类型服务对计算能力的需求很低。人脸识别服务在上传视频帧阶段会消耗大量的带宽资源，这取决于相机的分辨率和感兴趣的用例（例如，安全和监视或访问控制），但本文更关心的是人脸识别服务的存储要求和计算需求，人脸识别服务会消耗大量的计算资源（高达 $2~GHz$）和存储资源（至少 $2~GB$）来匹配可能有数千帧的数据库。以上关于服务参数设置值均与实际服务规范相符合。压缩服务计算需求设置在 $[0.04,0.32]~GHz$ 内，即处理 1 bit 数据需要 320 个 CPU cycles（或至少每 bit 需要 40 个 CPU cycles），而压缩服务对存储资源占用很小。增强现实也是需要资源的服务之一，它对计算资源的需求类似于人脸识别服务，而对存储资源的需求可能超过 $10~GB$。~~

（3）网络中其它参数

本节（1）中说明了基站的位置坐标为固定值，不随时间的变化而变化，也说明了移动终端在初始时刻均匀分布在网络区域内，但终端具有一定的移动性，每个终端都有自己的运动参数，包括位置、速度和运动方向。为简单起见，本文假设所有移动终端做匀速直线运动，且在每次模拟运行中，所有终端在基站的覆盖区域内呈均匀分布，其速度和方向服从一定区间内的均匀分布。本文设置移动终端速度服从 $[1,20]~m/s$ 的均匀分布，移动终端运动方向服从 $[\pi/4,\pi]$ 的均匀分布，从而可计算出 $T_{ij}^l$。

在模拟边缘网络上行/下行传输时，本文设置信道平均带宽 $W= 1~MHz$，噪声功率 $\sigma_{ij}^{2}=\sigma_{ji}^{2}=\sigma^{2}=-100~dBm$，$N_0=-200~dBm$，信道增益 $G_{ij} = \varpi_{ij} d_{ij}^{-\alpha}$，$\alpha=2$ 为路径损耗因子，$\varpi_{ij}$ 为信道增益系数（常数），$d_{ij}$ 表示移动终端 $u_{i}(u_{i}\in U_{1})$ 和服务器 $b_{j}(b_{j}\in\{B\cup U_{1}\})$ 在终端产生请求时刻的距离。由于终端具有移动性，距离 $d_{ij}$ 处于动态变化中，可能离卸载目的节点更近，可能离卸载节点更远，因此，出于全局考虑，本文信道增益中的距离以终端产生服务请求时的初始距离为准。另外，终端任务模型中上传数据大小 $I_{i}$ 和回传数据大小 $O_{i}$ 取决于具体请求的服务类型，例如，若对应服务请求类型属于视频流服务，则上传数据很小，回传数据很大。此外，本文将任务模型中的任务最大可容忍时延 $\tau_i$ 设置为服从 $[1,5]~s$ ==这个最后需要改成ms级别==的均匀分布。

综上所述，在仿真实验中使用的主要相关参数如表 3.3 所示。

表 3.3 实验参数设置表

| 參数                     | 值         |
|------------------------|-----------|
| 边缘服务器数量($M$)             | $9$         |
| 基站通信半径( $\mathcal{R}$) | $150~m$    |
| 移动终端数量( $N$ )            | $500$       |
| 信道平均带宽($W$)              | $1~MHz$    |
| 噪声功率 ($\sigma^2$)       | $-100~dBm$ |
| 轨迹可预测时间 ($T^p$)         | $4~s$      |
| 容忍时延($\tau_i$)          | $[1,5]~s$  |
| 边缘服务器最大计算能力 ($F_j$)    | $20~GHz$   |
| 边缘服务器最大存储能力($R_i$)       | $100~GB$   |


#### 3.1.3 性能指标

为优化边缘网络的性能，本文联合优化孪生模型部署决策、卸载决策和资源分配决策这三类决策，提出一种启发式计算卸载与服务优化算法 HCASO 和基于时延梯度的计算资源分配算法 DGRA，以最小化终端平均响应时延。从第三章优化问题 $\mathcal{P}$ 定义可知，平均响应时延是算法 HCASO 主要优化的目标，而平均响应时延表达式的分母由总卸载终端数量表示，分子由总卸载终端响应时延组成，最小化终端平均响应时延在某种意义上也可认为是最大化卸载终端数量和最小化卸载终端总响应时延。因此，为评估 HCASO 和 DGRA 算法的性能，本文主要考虑以下性能指标，分别是信息年龄、平均响应时延、本地和卸载执行时间比、卸载终端数量、总卸载数据量、资源利用率等。

**（1）信息年龄：** 本文用 ATE（Age of Information, AoI）表示，计算如公式（3.1）所示，根据 AoI 函数的增减性可知，AoI 值越小，表示终端对应孪生模型的信息年龄越低，即信息越新鲜，信息质量越高，HCASO 算法性能越好。
$$
\begin{align}
  \min_{\mathbf{a},\mathbf{b},\mathbf{c}} 
  \frac{\sum_{u_i \in U_1} \sum_{b_j \in \{B \cup U_2\}} a_{ij}  (\Delta + T_{ij}^{E})} 
{\sum_{u_i \in U_1} \sum_{b_j \in \{B \cup U_2\}} a_{ij}} .  \tag{3.1}
\end{align}
$$
**（2）本地执行和卸载执行时间比：** 本文用 TLE（Time Ratio of Local and Edge Execution, TLE）表示，计算如公式（3.3）所示。由于终端数量的快速增长，资源受限的网络难以对所有终端的服务请求进行响应，对于未响应服务请求的终端，即不进行端至边缘卸载和端到端卸载的终端，它们在本地执行任务，但其 QoS 得不到保证，本地执行的响应时延大于卸载执行的响应时延。如前所述，$T_i$ 表示终端 $u_i$ 执行任务的总时延，若终端在本地自身设备执行任务，则 $T_{i}^{L}=T_{i}$ 成立；若终端将任务卸载到边缘/微服务器执行，则 $T_i^L-T_i$ 越大越好。$T_{i}^{L}$ 与 $T_{i}$ 相差越大，则说明卸载执行比本地执行所花费时延更低，即 TLE 越大越好。
$$
\begin{align}
  TLE=\sum_{u_i\in U_1}\frac{T_i^L-T_i}{T_i^L}.  \tag{3.3}
\end{align}
$$

**（3）卸载终端数量：** 本文用 NTE（Number of Terminals Executed at the Edge, NTE） 表示，计算如公式（3.4）所示。正如性能指标平均响应时延指出，式（3.1）中分母由卸载终端数量表示，此外，从性能指标本地执行和卸载执行时间比中描述可知，资源受限的网络难以满足随终端数量日益增长的服务请求，因此，本文将卸载终端数量也作为性能评估指标之一。
$$
\begin{align}
  NTE=\sum_{u_i\in U_1}\sum_{b_j\in\{B\cup U_2\}}a_{ij}\:.  \tag{3.4}
\end{align}
$$

**（4）总卸载数据量：** 本文用 NOE（Number of the Offloading Data at the Edge,NOE） 表示，计算如公式（3.5）所示。对网络运营商而言，终端卸载的数据量越多，边缘服务器在执行卸载任务过程中所耗费的网络资源也会越多，相应地，网络运营商会获得更高的效益。
$$
\begin{align}
  NOE=\sum_{u_i\in U_1}\sum_{b_j\in\{B\cup U_2\}}a_{ij}I_i\:.  \tag{3.5}
\end{align}
$$

**（5）资源利用率：** 本文联合优化孪生模型部署决策、卸载决策和资源分配决策三类决策，孪生模型部署决策主要涉及到边缘服务器的存储资源，卸载决策和计算资源分配决策主要涉及到边缘服务器的计算资源，资源的有效管理可提高边缘网络的性能。在如今终端数量快速增长的环境下，存储/计算资源利用率的高低，可间接反映 HCASO 算法的性能。



### 3.2 对比算法

在仿真实验中，本文将所提出的启发式动态计算卸载与服务优化算法 HCASO 与以下五种算法进行比较：

**（1）就近卸载算法（NeighboringOfloading，NO）：** 使用本文提出的 HCASO 算法所得到孪生模型部署决策，~~在此条件下，移动终端将任务卸载到满足约束（1.12）~~，并选择离终端最近的边缘服务器卸载。

**（2）随机卸载算法（RandomOffloading，RO）：** 类似基准算法 NO，使用本文所提 HCASO 算法得到的孪生模型部署决策，而移动终端将在可行卸载目的节点集中随机选择一个可用的卸载目的节点进行任务卸载。

**（3）贪心算法（Greedy）：** 在该对比算法中，不同于 NO 与 RO，在存储资源限制内，Greedy 迭代地将模型放置在边缘服务器缓存中，最大限度提高边缘网络服务请求响应率，直至所有缓存都被填满，得到孪生模型部署决策，而移动终端进行任务卸载时选择就近卸载。

**（4）线性松弛算法（LinearRelaxation，LR）：** 该对比算法 LR 主要通过将 0-1 离散变量松弛为 0 到 1 之间的连续变量，并使用线性规划求解器求解以最大化卸载终端数量的计算卸载问题，得到孪生模型部署和计算卸载决策的最优分数解，以提供最优整数解的上界。线性规划求解器采用 MATLAB 优化工具箱中的 linprog 函数。

**（5）随机舍入算法（RandomRounding，RR）：** 该对比算法 RR 主要在线性松弛算法 LR 的基础上进行随机舍入，而其舍入的概率与 LR 得到的分数解相关。使用 LR 算法得到的分数解，虽然能保证满足资源限制，但分数解经过随机舍入为整数解后，会存在违反资源限制的可能，造成资源越界，若需要保证不违反资源约束，则需要对整数解二次处理，本文不予考虑。



## 3.3 性能评价

本节将从终端数量、边缘服务器计算和存储能力、输入数据和计算需求的变化来观察对 HCASO 算法产生的影响和变化，并根据多个性能指标评估 HCASO 算法的性能。

### 3.3.1 终端数量的影响

当其它变量处于基准值时，即服务器存储能力 $R=100~GB$、计算能力 $F=20~GHz$，服务数量 $K=100$ 等，本文讨论了移动终端数量 $N$ 对算法的影响。本文设置 $N$ 的最小值和最大值分别为 100 和 1000，从卸载终端平均响应时延 ATE、卸载终端数量 NTE、和总卸载数据量 NOE 三个指标，并结合其他对比算法，评估所提算法 HCASO 的性能。

首先，评估平均响应时延 ATE 随不同 $N$ 的变化，如图 3.2 所示，随着终端数量的增加，HCASO 的 ATE 值始终低于其它对比算法，即优于其它对比算法。当移动终端数量在 100 到 500 之间变化时，所有方案的 ATE 均呈上升趋势。当移动终端数量大于 500 时，由于边缘服务器计算资源限制，所有方案的 ATE 趋向于平衡、在平衡值小范围内上下浮动，其浮动原因在于终端产生不同种类的服务请求，且在所有方案中，HCASO 浮动变化最小。在 $N$ 大于等于 500 时，其中 RR 方案比 HCASO 的平均响应时延长约 $3.5\%$。
<div align="center" ><img src="https://fastly.jsdelivr.net/gh/wlchenGG/myPictureBed@main/images/20241010155210.png" alt="20241010155210" width="450" style="box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);border-radius:10px;"/><br><div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #777; font-size: 90%; padding: 1px;">图 3.2 ATE 随移动终端数量变化图</div></div>

然后，评估卸载终端数量 NTE 随不同 $N$ 的变化，如图 3.3 所示，从实验结果可以看出，随着终端数量的增加，HCASO 算法优于 Greedy、NO 和 RO，且 HCASO 算法的 NTE 更接近于 LR，LR 方案得到的为最优分数卸载决策解。由于 NO 和 RO 方案均使用 HCASO 方案得到服务部署决策，其卸载终端数量均与使用贪心服务部署的 Greedy 算法策交替优化算注的有性能不相上下，间接说明 HCASO 算法中的卸载决策与服务部署决策交替优化算法的有效性。
<div align="center" ><img src="https://fastly.jsdelivr.net/gh/wlchenGG/myPictureBed@main/images/20241010155903.png" alt="20241010155903" width="450" style="box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);border-radius:10px;"/><br><div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #777; font-size: 90%; padding: 1px;">图 3.3 NTE 随移动终端数量变化图</div></div>

其次，评估网络中总卸载数据量 NOE 随不同 $N$ 的变化，如图 3.4 所示，HCASO 算法的总体卸载数据量也更接近于 LR。与图 3.3 类似，由于卸载数据量跟终端所请求的服务相关，正如前面介绍的四种类型服务，在 MEC 服务器计算资源限制内，随着终端娄量增多，更多的服务请求得到响应，网络总卸载数据量也会增多，直到达到服务器计算资源瓶颈。
<div align="center" ><img src="https://fastly.jsdelivr.net/gh/wlchenGG/myPictureBed@main/images/20241010155945.png" alt="20241010155945" width="450" style="box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);border-radius:10px;"/><br><div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #777; font-size: 90%; padding: 1px;">图 3.4 NOE 随移动终端数量变化图</div></div>

最后，如前文所述，本文在考虑 MEC 卸载的同时，也考虑了少量终端的 D2D 卸载， 以扩展边缘网络的计算能力。图 3.5 比较了考虑 D2D 卸载的 HCASO 算法和未考虑 D2D 卸载的 HCASO 算法，结合图 3.2 可知，随着终端数量的增加，HCASO 算法的平均响应时延依然优于其它对比方案。
<div align="center" ><img src="https://fastly.jsdelivr.net/gh/wlchenGG/myPictureBed@main/images/20241010160003.png" alt="20241010160003" width="450" style="box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);border-radius:10px;"/><br><div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #777; font-size: 90%; padding: 1px;">图 3.5 ATE 随移动终端数量变化图</div></div>

对于前面提到的四种类型服务，即视频流、人脸识别、压缩、增强现实，本文在此说明 $K$ 种服务的服务类型分布，以及终端服务请求分布。设置服务数量 $K=300$、终端数量 $N=500$，图 3.6（a）描述了四种类型服务数量分别占总服务数量的比率，呈均匀分布，图 3.6（b）显示了当终端数量为 $N$ 时，网络中对四种服务类型的服务请求数分别占总终端数量的比例。
<div align="center" ><img src="https://fastly.jsdelivr.net/gh/wlchenGG/myPictureBed@main/images/20241010160235.png" alt="20241010160235" width="550" style="box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);border-radius:10px;"/><br><div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #777; font-size: 90%; padding: 1px;">图 3.6 服务类型和服务请求分布图</div></div>

与图 3.6 所设置的参数一致，图 3.7 描述了 HCASO 与 LR 两种算法对所响应服务请求的分布。如图 3.7 所示，HCASO 方案所响应 VS 服务请求的占比为 $23.76\%$，低于 LR 方案所响应 VS 服务请求的占比 $28.43\%$，而 HCASO 与 LR 算法所响应 FR 服务请求的占比分别为 $27.23\%$ 和 $21.73\%$。实验结果表明，使用 HCASO 算法时服务请求的响应分布更接近于图 3.6（b）中的终端服务请求分布，HCASO 算法更适用于多服务类型的网络环境。
<div align="center" ><img src="https://fastly.jsdelivr.net/gh/wlchenGG/myPictureBed@main/images/20241010160330.png" alt="20241010160330" width="550" style="box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);border-radius:10px;"/><br><div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #777; font-size: 90%; padding: 1px;">图 3.7 服务请求的响应分布</div></div>


## 3.3.2 MEC 服务器计算和存储资源的影响

本节评估 MEC 服务器计算和存储资源的变化对 HCASO 算法平均响应时延、卸载终端数量和总卸载数据量等性能指标的影响，同时评估在不同计算资源下，原子计算资源大小对 HCASO 算法的影响。

- **（1）计算资源**

在评估计算资源变化对 HCASO 算法的性能影响时，设置终端数量 $N=800$，服务数量与存储资源参数设置为基准值，即与图 3.2 设置一致。注意，在此次实验中并未执行基于时延梯度的计算资源分配算法 DGRA。

首先，评估平均响应时延随边缘服务器计算资源的变化，如图 3.8 所示，从实验结果可以看出，随着计算资源的增多，所有方案的 ATE 均呈下降趋势，HCASO 算法的 ATE 优于其它对比算法。当 MEC 服务器计算资源为 $20~GHz$ 时，对于卸载任务的终端而言， RR 比 HCASO 平均响应时延长约 $4.4\%$。当 MEC 服务器计算资源高于 $50~GHz$ 时，所有方案的 ATE 曲线走势趋于平缓，主要原因是虽然计算资源突破资源限制，可充分响应更多终端的服务请求，但终端的服务请求类型有限，即服务对计算资源需求的类型有限， 从而导致平均响应时延没有明显的下降趋势。在计算资源为 $60~GHz$ 时，RO 的 ATE 低于 HCASO，这是由于 RO 算法是在 HCASO 算法所得到的服务部署上进行的随机卸载，具有一定的随机性。
<div align="center" ><img src="https://fastly.jsdelivr.net/gh/wlchenGG/myPictureBed@main/images/20241010162006.png" alt="20241010162006" width="450" style="box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);border-radius:10px;"/><br><div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #777; font-size: 90%; padding: 1px;">图 3.8 ATE 随计算资源变化图</div></div>

然后，评估卸载终端数量 NTE 随边缘服务器计算资源的变化，如图 3.9 所示，从实验结果可以看出，随着计算资源的增大，HCASO 方案的卸载数量更接近于 LR 和 RR 方案，且优于 RO、NO 和 Greedy 方案，RR 方案得到的解可能会违背资源约束。从图中可以看出，随着服务器计算资源的增多，所有方案的 NTE 都有所提高，几乎呈线性上升趋势，其原因在于计算资源的增长可使边缘服务器响应更多的终端服务请求。
<div align="center" ><img src="https://fastly.jsdelivr.net/gh/wlchenGG/myPictureBed@main/images/20241010162121.png" alt="20241010162121" width="450" style="box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);border-radius:10px;"/><br><div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #777; font-size: 90%; padding: 1px;">图 3.9 NTE 随计算资源变化图</div></div>

最后，评估网络总卸载数据量 NOE 随边缘服务器计算资源的变化，如图 3.10 所示， 从实验结果可以看出，随着计算资源的增多，HCASO 方案的 NOE 更接近于 LR，LR 得到的是最优分数解，并非二进制解。结合图 3.10 与图 3.9，可以观察到，所有方案的 NOE 曲线与 NTE 曲线走势类似，这是因为当计算资源大小固定时，响应的终端服务请求走多，网络的总卸载数据量越多，所以当计算资现增多时，NOE 呈现出和 NTE 曲线相似的上升趋势。
<div align="center" ><img src="https://fastly.jsdelivr.net/gh/wlchenGG/myPictureBed@main/images/20241010162132.png" alt="20241010162132" width="450" style="box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);border-radius:10px;"/><br><div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #777; font-size: 90%; padding: 1px;">图 3.10 NOE 随计算资源变化图</div></div>

- **（2）计算资源原子块大小**

为了验证基于时延梯度的计算资源分配算法 DGRA 的有效性，本文针对不同的计算资源和不同的计算资源原子块大小，评估平均响应时延 ATE 和本地/卸载执行时间比 TLE 对执行 DGRA 的 HCASO 算法的影响。这里设置终端数量 $N=500$，其余参数设置为基准值。
<div align="center" ><img src="https://fastly.jsdelivr.net/gh/wlchenGG/myPictureBed@main/images/20241010162356.png" alt="20241010162356" width="450" style="box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);border-radius:10px;"/><br><div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #777; font-size: 90%; padding: 1px;">图 3.11 ATE 随计算资源原子块变化图</div></div>

首先，评估平均响应时延 ATE 随不同计算资源原子块的变化，如图 3.11 所示，从实验结果可知，在固定边缘服务器计算资源大小时，计算资源原子块越小，其 ATE 值越小；在固定计算资源原子块大小时，计算资源越充足，其 ATE 值也越小。当原子块大小为 $0.05GHz$ 时，在 HCASO 方案中， $F=40GHz$ 的 ATE 值比 $F=10GHz$ 的 ATE 值约低 $12.3\%$。当 $F=40GHz$ 时，在 HCASO 方案中，原子块大小为 $0.05GHz$ 的 ATE 值比原子块大小为 $0.15GHz$ 的 ATE 值约低 $3.8\%$。
<div align="center" ><img src="https://fastly.jsdelivr.net/gh/wlchenGG/myPictureBed@main/images/20241010162420.png" alt="20241010162420" width="450" style="box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);border-radius:10px;"/><br><div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #777; font-size: 90%; padding: 1px;">图 3.12 TLE 随计算资源原子块变化图</div></div>

然后，评估本地/卸载执行时间比 TLE 随不同计算资源原子块的变化，如图 3.12 所示，从实验结果可看出，当边缘服务器计算资源大小固定时，计算资源原子块越小，其 TLE 值越大，由本章 5.2 节性能指标介绍可知，TLE 值越大，说明性能越好；当计算资源原子块大小固定时，边缘服务器计算资源越大，TLE 值越大。

结合图 3.11 和 5.12 可知，计算资源原子块越小，其对降低卸载时延的贡献越大， 但是由于原子块越小，DGRA 算法时间复杂度越高，因此，在大规模网络场景中，需要选择合适的原子块大小以平衡时间开销。此外，基于时延梯度的计算资源分配算法主要针对拥有剩余可用计算资源的边缘服务器，即该算法更适用于计算资源比较充足的边缘网络。

- **（3）存储资源**

在分析边缘服务器存储资源变化对 HCASO 算法的性能影响时，由于存储资源主要与服务数量 $K$ 有关，于是设置服务数量 $K=1000$，并设置终端数量 $N=800$，计算资源设置为基准值 $F=20GHz$。

首先，评估平均响应时延 ATE 随不同边缘服务器存储资源的变化，如图 3.13 所示， 随着边缘服务器存储资源的增多，HCASO 的 ATE 值也逐渐上升，但依然优于其它对比算法。在存储资源小于 $150GB$ 时，HCASO 与 NO、RO、Greedy 方案的 ATE 曲线呈上升趋势，其原因在于随着存储资源的增多，边缘服务器能部署更多的服务，也能响应更多终端的服务请求，但由于终端服务请求呈现多样化，不同的服务拥有不同的容忍时延， 从而导致 ATE 曲线呈上升趋势。在存储资源大于 $150GB$ 时，由于计算资源的限制，网络能响应的终端服务请求的数量有限，所以所有方案的 ATE 曲线逐渐趋于平缓状态。
<div align="center" ><img src="https://fastly.jsdelivr.net/gh/wlchenGG/myPictureBed@main/images/20241010162637.png" alt="20241010162637" width="450" style="box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);border-radius:10px;"/><br><div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #777; font-size: 90%; padding: 1px;">图 3.13 ATE 随存储资源变化图</div></div>

然后，评估卸载终端数量 NTE 随不同边缘服务器存储资源的变化，如图 3.14 所示， 从图中可看出，随着边缘服务器存储资源逐渐增多，所有方案的 NTE 变化趋势均先上升，然后趋于平缓，其中 HCASO 方案最接近于 LR 与 RR 方案，RR 方案得到的解会存在违背资源约束的情况。所有方案的 NTE 曲线在最开始处于上升阶段是由于边缘服务器计算资源处于充足状态，NTE 曲线最后处于平缓阶段是因为边缘服务器计算资源有限，不能响应更多的终端服务请求。
<div align="center" ><img src="https://fastly.jsdelivr.net/gh/wlchenGG/myPictureBed@main/images/20241010162739.png" alt="20241010162739" width="450" style="box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);border-radius:10px;"/><br><div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #777; font-size: 90%; padding: 1px;">图 3.14 NTE 随存储资源变化图</div></div>

最后，评估网络中总卸载数据量 NOE 随不同边缘服务器存储资源的变化，如图 3.15 所示，从实验结果可看出，图 3.15 中的 NOE 曲线走势与图 3.14 中的 NTE 曲线走势大体一致，其原因在于随着存储资源的增加，响应的终端数量也逐渐增加，从而总卸载数据量也增加，直到达到计算资源的限制，NOE 曲线逐渐趋于平稳。
<div align="center" ><img src="https://fastly.jsdelivr.net/gh/wlchenGG/myPictureBed@main/images/20241010165330.png" alt="20241010165330" width="450" style="box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);border-radius:10px;"/><br><div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #777; font-size: 90%; padding: 1px;">图 5.15 NOE 随存储资源变化图</div></div>

由于存储资源主要用于部署服务，为了分析在不同的存储资源情况下，不同服务数量 $K$ 对 HCASO 算法的性能影响，本文观察了在其它参数不变、不同的 $K$ 值对 HCASO 算法平均响应时延 ATE 的影响，即 $K=100$、 $K=300$、 $K=1000$，如图 3.16 所示，在计算资源限制内，当服务数量 $K$ 固定时，ATE 值随着存储资源的增多而增加；当存储资源固定时，ATE 值也随着服务数量 $K$ 的增多而增加。
<div align="center" ><img src="https://fastly.jsdelivr.net/gh/wlchenGG/myPictureBed@main/images/20241010165350.png" alt="20241010165350" width="450" style="box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);border-radius:10px;"/><br><div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #777; font-size: 90%; padding: 1px;">图 3.16 ATE 在不同 $K$ 下随存储资源变化图</div></div>

- **（4）资源利用率**

为了评估 HCASO 算法和部分对比算法在边缘网络下的存储/计算资源利用率，本文将所有参数设置为基准值进行实验仿真，得到各边缘服务器在不同算法下的存储/计算资源利用率，如图 3.17 所示。注意，除了计算资源原子块大小对 HCASO 算法的实验仿真，其它实验均未使用基于时延梯度的计算资源分配算法。
<div align="center" ><img src="https://fastly.jsdelivr.net/gh/wlchenGG/myPictureBed@main/images/20241010165415.png" alt="20241010165415" width="450" style="box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);border-radius:10px;"/><br><div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #777; font-size: 90%; padding: 1px;">图 3.17 资源利用率</div></div>

图 3.17（a）显示 HCASO、Greedy 和 RR 方案的存储资源利用率，使用 HCASO 方案的边缘服务器平均存储资源利用率达到 $93.12\%$，第 5 台边缘服务器使用 HCASO 方案的存储资源率低于其它方案，其原因在于 HCASO 主要根据终端的服务请求制定部署决策以缓存服务，对于剩余的可用存储资源，可用于缓存其它流行度较高的服务，以响应未来终端可能产生的服务请求并减少服务迁移成本。从图 3.17（a）中可知，使用 RR 方案的第 1 台和第 9 台边缘服务器的存储资源利用率均超过资源上限，即前文提到的 RR 方案虽然接近于 LR，但会存在违背资源约束的问题。同理，图 3.17（b）中使用 RR 方案的部分服务器也出现违背资源约束问题。使用 HCASO 方案的边缘服务器平均计算资源利用率为 $98.84\%$，Greedy 的平均计算资源利用率为 $94.65\%$，实验表明 HCASO 方案能够充分地利用计算资源。
<div align="center" ><img src="https://fastly.jsdelivr.net/gh/wlchenGG/myPictureBed@main/images/20241010165433.png" alt="20241010165433" width="450" style="box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);border-radius:10px;"/><br><div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #777; font-size: 90%; padding: 1px;">图 3.18 存储资源利用率</div></div>

如前文所述，本文考虑了服务实体与微服务实体关系，一个服务实体由 $n$ 个微服务实体编排而成，这里设置 $n=4$。为了评估使用微服务部署对网络性能的提升，本文对使用服务部署和微服务部署两种方式的 HCASO 算法进行存储资源利用率评估，服务数量设置 $K=300$，微服务数量设置为基准值 $T=400$，两种方式对应的边缘服务器存储资源利用率情况如图 3.18 所示。从图中可看出，使用微服务部署的 HCASO 算法的存储资源利用率整体低于使用服务部署的 HCASO 算法的存储资源利用率，且前者平均存储资源利用率为 $83.15\%$，后者平均存储资源利用率为 $98.72\%$，即前者节省了更多存储资源， 其原因在于 HCASO 使用服务部署时会重复部署已部署的微服务，从而导致存储资源占用率高。简言之，在服务器端进行微服务部署可以节省大量存储资源，此外，通过使用服务编排技术将微服务编排为服务，节省的存储资源可用于部署更多微服务，以响应终端的多样化服务请求。

### 3.3.3 输入数据和计算需求的影响

如前文所述， $K$ 种不同类型的服务均匀映射到视频流、人脸识别、压缩和增强现实四种服务类型，这四种类型服务具有不同的计算能力需求，且对于不同的请求服务类型， 终端进行任务卸载时的输入数据大小（即需要上传的数据大小）也会存在差异。因此，本节分析各服务的输入数据和计算需求的变化以及不同分布对 HCASO 算法的影响。本次实验参数设置终端数量 $N=800$，其它参数设置为基准值。

- **（1）服务输入数据大小**

每种类型服务的输入数据大小默认在一定区间内呈均匀分布，VS、FR、Gzip 和 AR 四种类型服务的输入数据分别呈 $[0,5]~Mb$、$[1, 5]~Mb$、$[2, 4]~Mb$、$[2, 7]~Mb$ 区间中的均匀分布。在本次实验仿真中，增加了输入数据呈正态分布的实验，并设置正态分布的均值与标准差分别为呈均匀分布的均值和 1。接下来分析当输入数据为两种分布并分别为原输入数据倍数时对 HCASO 算法的影响。
<div align="center" ><img src="https://fastly.jsdelivr.net/gh/wlchenGG/myPictureBed@main/images/20241010165531.png" alt="20241010165531" width="700" style="box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);border-radius:10px;"/><br><div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #777; font-size: 90%; padding: 1px;">图 3.19 ATE 随服务输入数据变化图</div></div>

首先，评估平均响应时延 ATE 随不同输入数据大小的变化，如图 3.19 所示，从实验结果可以看出，无论输入数据呈均匀分布还是正态分布，所有方案的 ATE 值都随着输入数据的增大而增大，这是因为在计算资源需求固定时，其输入数据越大，响应时延也会越大，但在所有方案中，HCASO 方案的 ATE 值低于处于同一条件下其它方案的 ATE 值，即 HCASO 优于其它对比算法。由图 3.19（a）可看出，当输入数据呈均匀分布、输入数据为原输入数据的 1.25 倍时，HCASO 的 ATE 值约为 2.47 s，比处于同一状态下 NO 的 ATE 值低 $9.53\%$ ；由图 3.19（b）可看出，当输入数据呈正态分布、输入数据为正态分布的原输入数据的 1.25 倍时，HCASO 的 ATE 值约为 2.50 s，高于 HCASO 方案呈均匀分布、并为原输入数据 1.25 倍的 ATE 值。

然后，评估卸载终端数量 NTE 随不同输入数据大小的变化，如图 3.20 所示，从图中可看出，随着输入数据的增大，所有方案的 NTE 曲线均呈下降趋势，其原因在于边缘服务器计算资源有限，但 HCASO 的 NTE 值更接近于 LR 和 RR。从图 3.20（a）可知，当输入数据呈均匀分布且为原输入数据大小的 0.5 倍时，HCASO 方案的 NTE 高于 Greedy 约 $16.96\%$，低于 LR 约 $9.47\%$。从图 3.20（b）可看出，当输入数据呈正态分布且为原输入数据大小的 0.5 倍时，HCASO 方案的 NTE 高于 Greedy 约 $22.79\%$，低于 LR 约 $11.05\%$。结合图 3.20（a）和图 3.20（b）可知，当输入数据呈正态分布时，HCASO 的 NTE 高于在同一条件下输入数据呈均匀分布的 HCASO 的 NTE。
<div align="center" ><img src="https://fastly.jsdelivr.net/gh/wlchenGG/myPictureBed@main/images/20241010165630.png" alt="20241010165630" width="700" style="box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);border-radius:10px;"/><br><div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #777; font-size: 90%; padding: 1px;">图 3.20 NTE 随服务输入数据变化图</div></div>

最后，评估网络中总卸载数据量 NOE 随不同输入数据大小的变化，如图 3.21 所示。注意，由于本文考虑终端任务的整体卸载，输入数据即卸载数据，在其它网络资源充足的情况下，总卸载数据量与输入数据大小约成正比关系。从图 3.21 可知，无论输入数据呈均匀分布还是正态分布，各方案的总卸载数据量都随着输入数据增大而增大，HCASO 依然最接近于 LR 和 RR。结合图 3.21 和图 3.20 可知，输入数据越大，表示终端任务的计算能力需求量也越大，然而在边缘服务器计算资源固定的情况下，由于输入数据的增长速度大于 NTE 值的下降速度，所以总卸载数据量随输入数据增大而增多。
<div align="center" ><img src="https://fastly.jsdelivr.net/gh/wlchenGG/myPictureBed@main/images/20241010165719.png" alt="20241010165719" width="700" style="box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);border-radius:10px;"/><br><div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #777; font-size: 90%; padding: 1px;">图 3.21 NOE 随服务输入数据变化图</div></div>

- **（2）服务计算需求**

由表 3.2 可知，不同类型的服务对计算资源有着不同的需求，在前面实验中，服务的计算需求按照表 3.2 设置，且呈均匀分布，这里将单位转换为 CPU cycles/bit。在本次实验仿真中，分别讨论了服务计算需求呈均匀分布和正态分布时对 HCASO 算法的影响，当计算需求呈正态分布时，将其均值与标准差都设置为均匀分布的均值。
<div align="center" ><img src="https://fastly.jsdelivr.net/gh/wlchenGG/myPictureBed@main/images/20241010165840.png" alt="20241010165840" width="700" style="box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);border-radius:10px;"/><br><div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #777; font-size: 90%; padding: 1px;">图 3.22 ATE 随服务计算需求变化图</div></div>

首先，评估平均响应时延 ATE 随服务不同计算需求的变化，如图 3.22 所示，从实验结果可知，无论服务计算需求呈均匀分布还是正态分布，HCASO 的 ATE 值均低于处于同一条件下的其它方案的 ATE 值。所有方案的 ATE 值随着计算需求的增加而增大， 其原因在于边缘服务器可用计算资源有限，为每个终端分配的计算资源也有限。从图 3.22（a）中可看出，当计算需求呈均匀分布且为原计算需求的 1.75 倍时，HCASO 的 ATE 值低于 RR 约 $2.51\%$。从图 3.22（b）可知，当计算需求呈正态分布且为原计算需求的 1.75 倍时，HCASO 的 ATE 值低于 RR 约 $4.31\%$。
<div align="center" ><img src="https://fastly.jsdelivr.net/gh/wlchenGG/myPictureBed@main/images/20241010170140.png" alt="20241010170140" width="700" style="box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);border-radius:10px;"/><br><div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #777; font-size: 90%; padding: 1px;">图 3.23 NTE 随服务计算需求变化图</div></div>

然后，评估卸载终端数量 NTE 随不同计算需求的变化，如图 3.23 所示，从图中可看出，随着服务计算需求的增长，所有方案的 NTE 曲线均呈下降趋势，HCASO 依然最接近于 LR 和 RR。从图 3.23（a）可知，当服务计算需求呈均匀分布且为原计算需求的 1.25 倍时，HCASO 的 NTE 高于 Greedy 约 $15.64\%$。从图 3.23（b）可知，当服务计算需求呈正态分布且为原计算需求的 1.25 倍时，HCASO 的 NTE 高于 Greedy 约 $15.44\%$，高于处于均匀分布且为原计算需求 1.25 倍下 HCASO 的 NTE 约 $6.12\%$。
<div align="center" ><img src="https://fastly.jsdelivr.net/gh/wlchenGG/myPictureBed@main/images/20241010170210.png" alt="20241010170210" width="700" style="box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);border-radius:10px;"/><br><div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #777; font-size: 90%; padding: 1px;">图 3.24 NOE 随服务计算需求变化图</div></div>

最后，评估网络中总卸载数据量 NOE 随服务不同计算需求的变化，如图 3.24 所示， 从实验结果可以看出，所有方案的 NOE 值均随着服务计算需求的增加而减少，HCASO 方案接近于 LR 和 RR。从图 3.24（a）可知，随着计算需求的增加，HCASO 的 NOE 值与 LR 的 NOE 值之间的差距逐渐增大，但依然优于 Greedy、NO 和 RO 三种方案。当服务计算需求呈均匀分布且为原计算需求的 0.75 倍时，虽然 HCASO 的 NOE 低于 LR 的 NOE 约 $14.29\%$，但 HCASO 的 NOE 高于 Greedy 的 NOE 约 $29.18\%$。从图 3.24（b）可知，当服务计算需求呈正态分布且为原计算需求的 1.5 倍时，HCASO 的 NOE 低于 LR 的 NOE 约 $20.94\%$，高于 NO 的 NOE 约 $25.73\%$。结合图 3.24 和图 3.23，可观察到两图中所有方案对应的曲线变化趋势相似，这是因为边缘服务器计算资源有限，当服务计算需求增大而其它参数不变时，响应的终端服务请求数量下降，从而导致总卸载数据量下降。

