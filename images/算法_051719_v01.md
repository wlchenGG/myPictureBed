

Paper-24-02



## 数字孪生赋能车载边缘计算中**Age-of-Information-Aware协作计算卸载** 

for Monitoring and Control Applications 



Age of Information-Aware Cooperative Computation Offloading for Monitoring and Control Applications in Digital Twin-Empowered Vehicular Edge Computing 

（Decentralized Bilevel Learning）


##   ==双层学习==问题模型

目标函数可以修改

目标函数的上下层分割可以修改

- 双层优化
- 时延梯度的想法可以保留
- 最好结合深度学习相关的点

对等网络上的去中心化双层学习



在开发用于去中心化双层优化的网络一致性方法时，出现了两个基本挑战：首先，紧密耦合的内外数学结构，加上外部问题的去中心化性质和非凸性，使得算法的设计和理论分析比解决传统的单层最小化问题更具挑战性。其次，由于地理分散的边缘网络中潜在的低速网络连接和随机梯度型信息的巨大可变性，用于分散双层优化的网络一致性方法对通信和样本复杂性非常敏感。

![image-20240506170154040](/Users/xiangyi/Library/Application Support/typora-user-images/image-20240506170154040.png)



![image-20240506170253338](/Users/xiangyi/Library/Application Support/typora-user-images/image-20240506170253338.png)

==其中，T_{ij}^E是总延迟（包括计算延迟+通信延迟+排队延迟）==

==T_{ij}^E是计算延迟==

* 上层决策：服务部署
* 下层决策：计算卸载与资源分配



在协作智能缓存中，内容缓存和交付需要在不同的时间尺度上运行。内容缓存的过程中边缘服务器从主干网检索内容并替换会带来不可忽视的延迟和成本，另一方面，考虑到内容的流行度在一个大的时间段内平缓变化，而用户的请求在每个时隙都会发生，因此为了避免频繁缓存替换而带来不必要的开销，本文考虑在不同的时间尺度进行内容缓存与内容交付的决策。分层的MADDPG算法在不同的时间尺度上交替进行，重用上层策略并根据不同的时间尺度设计优化的目标，以实现有远见的内容缓存决策。


<div align="center"><img src="https://fastly.jsdelivr.net/gh/wlchenGG/myPictureBed@main/images/20240508100056.png" alt="20240508100056" width="700px" align="center"/></div>

==图 双时间尺度优化服务部署、计算卸载与资源分配==



为了求解提出了双层优化问题提出了使用多代理的分层双时间尺度深度强化学习方法。另一方面针对D2D网络中用户共享内容的社交性，提出了基于用户偏好的节点聚类算法，以分布式的求解网络中的内容缓存与内容交付问题；最后针对深度强化学习模型训练提出多智能体边缘联邦学习聚合算法。



![image-20240506214556925](/Users/xiangyi/Library/Application Support/typora-user-images/image-20240506214556925.png)



* 算法执行流程

针对目标问题，本节对求解问题的算法总框架进行叙述。

（1）首先观察缓存节点覆盖范围内的用户对服务的偏好情况，并根据偏好情况以及用户的实时位置对用户进行聚类。由于不同的用户具有不同的偏好情况，每一类用户具有自己独特的属性，对用户进行聚类可以更好的进行服务部署决策，用户可以分布式的在同类用户范围处部署服务，更好的支持计算卸载决策。

（2）其次基于分层MADDPG算法的联合优化内容缓存与内容交付决策，内容缓存决策与内容交付决策使耦合的，首先针对耦合的问题进行分解，然后使用分层MADDPG针对分层的内容缓存与内容交付问题进行嵌套的联合的求解，在满足缓存容量和完成交付的限制下，最大化网络内的内容共享增益和内容获取增益。

（3）最后为了达成全局最优的结果，减少代理通信消耗同时共享代理间的信息，在边缘进行动态的联邦学习聚合方法，在每个学习时期，所有的边缘代理共享其actor网络参数并进行联邦更新。根据通过用户聚类结果动态的进行局部参数聚合并为了获取全局信息进行全局聚合，加速分层MADDPG算法的训练过程。

采用分层的MADDPG算法，上层与下层的MADDPG算法嵌套进行，交替优化，联合训练上下层的MADDPG神经网络。



![image-20240506203817131](/Users/xiangyi/Library/Application Support/typora-user-images/image-20240506203817131.png)

![image-20240506204131087](/Users/xiangyi/Library/Application Support/typora-user-images/image-20240506204131087.png)


#### 流程

##### Matlab 原流程
- 基本参数设置
- 0.全局解变量
- 1.剪枝 - 获取可卸载目的节点集和个数
    while （多次迭代取最优）
        2.构建初始计算卸载决策
        3.构建服务部署决策，更新卸载决策
        4.评估本次解决方案：保留最好的方案
    end while
- 5.分配计算资源

以上流程中，1-5 步骤就是整个决策过程，需要替换成我们新的 双层MADDPG 的过程，如下：

- 基本参数设置
- 0.全局解变量
- FOR t:1:T // 总执行时隙
  - // ~~我们先不考虑聚类~~
  - FOR $a \in A$
    - // 上层决策
    - IF $t \% \tau == 0$ // 每 $\tau$ 个时隙执行一次
      -  获取当前上层状态
      -  选择上层动作
      -  $r^{up} \gets 0$
    - END IF
    - // 下层决策
    - FOR $t'=t:1:t+\tau$
      - 获取当前下层状态 + 当前上层动作
      - 选择下层动作
      - 下层网络执行动作，观测下一时刻状态、奖励 $r^{down}$，并记入缓存区
      - $r^{up} \gets r^{up} + r^{down}$
    - END FOR t'
  - END FOR a
  - IF $t\%\lambda == 1$
    - 更新目标Actor、Critic网络参数
  - END IF
  - // 定期参数聚合
  - 
- END FOR t


## Our numerical experiments 


#### 代码问题

1. 代理是否要分类？这涉及到训练后的多代理参数聚合。<font color=#00AF00>分类</font>。in `runner.py`
   - 如果不分类，那么参数聚合时，是否直接全局所有代理聚合，还是邻居代理聚合？ 是否需要考虑参数共享？
   - 如果分类，可以保留原来的 <font color=#FF0000>类内参数共享 + 全局参数聚合</font> 思路，为了区分可以换其他共享和聚合方法。
  <div align="center"><img src="https://fastly.jsdelivr.net/gh/wlchenGG/myPictureBed@main/images/20240507231527.png" alt="20240507231527" width="700px" align="center"/></div>

2. 上下层的网络结构是否需要不一样？暂时可以只改输入输出维度，因为上下层的状态和动作空间不一样。in `actor_critic.py`。
3. 协作用户数目是否要固定，还是说每个时隙中没有请求的用户就作为协作者？ 感觉后者符合实际些，但需要解决用户位置和请求随时间动态变化的问题，这会导致一个代理上各时隙的动作空间的变化，导致网络输入维度变化。或者可以以 one-hot 向量形式记录用户是请求或者协作者，实际上不是请求就是协作，



#### 需要改的地方

1. `runner.py` 中（算法主体）：
   1. In `__init__()` 算法构造函数：`Runner` 类的部分参数需要两套，一套用于上层，一套用于下层
   2. In `_init_agents()` 初始化代理函数：代理是否需要分类，分类则不变，否则合并成一类。<font color=#00AF00>分类</font>
   3. In `merge_fl()` 类内参数共享函数：<font color=#00AF00>保留，考虑到网络结构不同，分成两部分代码分别处理</font>
   4. In `run()` <font color=#FF0000>算法主函数</font>：
      1. 增加周期上层处理：
         - 获取上层观察（基站已部署(微)服务、基站剩余存储资源、协作终端已部署微服务、请求服务各项值），==是否考虑只能局部可观测？==，<font color=#00AF00>**观察范围仅限基站范围内的用户**，每个代理的网络输入状态大小就不是一致的，</font><font color=#FF0000>后续注意看看多时隙动态过程中，用户位置变化导致代理可观测状态的变化</font>
         - ActionUpper网络前向传播获取上层动作（服务部署）
         - ==问题：服务和微服务如何处理？参考matlab来？== <font color=#00AF00>不考虑微服务，只考虑不可分的服务（服务大小、计算负载等属性）</font>
      2. 按 Word 里算法思路，为下层增加一层循环，迭代 $\tau$ 个时隙，每个时隙处理：
         - 获取下层观察（基站已部署(微)服务、基站剩余存储/计算资源、协作终端已部署微服务、协作终端计算资源、请求服务各项值、用户位置、覆盖情况、<font color=#FF0000>上层输出的服务部署决策动作</font>）
         - ActionLower网络前向传播获取下层动作（计算卸载、计算资源分配）：==如何考虑时延梯度？从Action网络内还是外？损失函数上入手？== <font color=#00AF00>增加一个时延梯度向量作为附加启发式信息输入给网络</font>
         - 执行动作（部署、卸载、资源），观测下一时刻状态、奖励，并记入下层回放池
         - 如果下层回放池经验足够，则计算下层损失，更新下层 DDPG 参数。==同时是否需要进行参数共享和参数聚合== <font color=#00AF00>是上下层一起共享和聚合，因此应该上层经过周期次参数更新后</font>
         - 递增上层奖励
         - <u>记入上层回放池</u>
      3. 将下一时刻上层状态、$\tau$平均上层奖励计入上层回放池。==其中上层状态来自于下层执行 $\tau$ 后的观察，是否合理？毕竟上层所做的部署决策是基于 $t$ 时的状态，而后又经过 $\tau$ 个时隙的下层迭代后，得到的下一状态（实际是$t+\tau$后的状态）。**好像是没问题的。只是需要考虑如下图中问题：**==
      4. 如果上层回放池经验足够，则计算上层损失，更新下层 DDPG 参数
        <div align="center"><img src="https://fastly.jsdelivr.net/gh/wlchenGG/myPictureBed@main/images/20240508002421.png" alt="20240508002421" width="800px" align="center"/></div>
      
      5. ==是否需要进行参数共享和参数聚合？共享和聚合的周期分别为多少？目前简化流程，可以暂时不考虑共享，只在每次上层更新时，进行上下层的参数聚合== <font color=#00AF00>每经过一定次数的上层训练后，就执行一次类内参数共享，同时全局参数聚合。</font>
      6. 保存奖励和损失时间序列时，应该将上下层都保存，到时可以体现上下层的收敛情况。
   5. In `evaluate` ：~~次要，之后看看是否需要上下层都评价并记录 rewards~~

2. `env.py` 环境类：
   1. In `__init__()` 环境构造函数：
   2. In `reset()` 环境重置函数：
   3. In `step()` 环境步进函数：基于当前状态和动作，返回下一状态和奖励
      - 去掉 基站开关状态切换成本 相关
      - 去掉 服务迁移 相关
      - 更新 时延计算方法：==端协作时，数据传输时延如何考虑？通过覆盖基站转发还是终端 D2D 直连==
      - 统计各项指标：时延、存储资源、计算资源利用率、请求满足率、网络吞吐量
      - 汇总执行动作后的 下一步状态向量、奖励向量
3. `agent.py` 代理类：
   1. `__init__()`：已经改为初始化两个DDPG，`policy_upper` 用于上层，`policy_lower` 用于下层
   2. `softmax()`：不变
   3. `select_action()`：由于动作处理不一样，分成上层和下层函数
      1. `select_upper_action()`：动作定界较简单，动作 `pi` 大于 0.5 时决策就设为 1，否则决策设为 0。
      2. `select_lower_action()`：每个请求只能由覆盖基站和范围内协同终端之一提供卸载服务。==TODO:这个设定可能需要调整神经网络架构==
   4. `learn()`：不变
4. `maddpg.py`：
   1. `__init__()`：根据上下层指示参数，初始化上下层 actor 和 critic 网络
   2. `_soft_update_target_network()`：不变
   3. `train()`：不变
   4. `save_model()`：保存文件名标识上下层
5. `actor_critic.py` 网络结构类：分成上下层
   1. `class ActorUpper`：
   2. `class CriticUpper`：
   3. `class ActorLower`：
   4. `class CriticLower`：
6. `commmon/replay_buffer.py` 经验回放池类：
   1. `__init__()`：因为上下层的动作状态大小不同，因此区分上下层初始化
   2. `store_episode()`：不变
   3. `sample()`：不变
   4. `_get_storage_idx()`：不变
7. `commmon/arguments.py` 超参数设置：
   1. `get_args()`：增加上层训练周期参数 episode-upper=20，上层训练 batch-size-upper=16，因为下层是 256，尽量让两层训练频次接近
8. `utils.py` 工具函数：
   1. [x] `make_env()` 创建环境函数：区分设定上下层 观测和动作长度
      ``` 
          # 环境状态：每个基站范围内的 任务总大小 + 各任务大小 + 各计算强度 + 各计算容量 + 用户位置 + 协作用户部署服务
                                      1 100 100 100 100 2*100 10
          # 对于每个代理：
          # Upper观察：每个基站范围内的 任务总大小 + 各任务大小 + 各计算强度 + 各计算容量 + 各部署服务 + 用户位置 + 协作用户部署服务
          #                           1 100 100 100 100 2*100 10
          # Lower观察：每个基站范围内的 任务总大小 + 各任务大小 + 各计算强度 + 各计算容量 + 各部署服务 + 用户位置  + 协作用户部署服务 + Upper动作
          #                           1 100 100 100 100 2*100 10 100
          
          # Upper动作：服务部署决策动作（假设每个用户对应一种服务）{0,1}
          #                           100
          # Lower动作：计算卸载决策动作 {0,1} + 10个终端协作卸载决策 {0,1}、资源分配决策动作 [0,1]
          #                           11*100 + 100
      ```

#### 调试过程



##### FCSM_0511v1

<div align="center"><img src="https://fastly.jsdelivr.net/gh/wlchenGG/myPictureBed@main/images/20240515194552.png" alt="20240515194552" width="600px" align="center"/><br>10000_20x64x256</div>

##### FCSM_0512v1

In `agent.py`, 下层选动作的函数 `select_action_lower()`，增加了<font color=#FF0000>根据时延梯度优先级为终端分配计算资源</font>：考虑了卸载到基站和协作终端两种情形。可以看到，纵坐标值非常大，估计是添加的时延梯度代码引入了错误，又异常资源分配值，导致计算出的奖励值（`env.py` 中）非常大，进而导致损失非常大。

<div align="center"><img src="https://fastly.jsdelivr.net/gh/wlchenGG/myPictureBed@main/images/20240515195021.png" alt="20240515195021" width="600px" align="center"/><br>1024_10x32x256</div>

##### FCSM_0514v01_actor

<div align="center"><img src="https://fastly.jsdelivr.net/gh/wlchenGG/myPictureBed@main/images/20240515195313.png" alt="20240515195313" width="600px" align="center"/><br>1024_10x32x256</div>

##### FCSM_0515v01

时延梯度部分代码只保留了计算基站上的资源分配。

<div align="center"><img src="https://fastly.jsdelivr.net/gh/wlchenGG/myPictureBed@main/images/20240515200825.png" alt="20240515200825" width="600px" align="center"/></div>

##### FCSM_0516v01

为回放经验加时延梯度值，在批量采样时，时延梯度值越大的样本，被采样到的概率越大。需要改动：
1. `replay_buffer.py`：增加时延梯度值的定义和处理
   - `__init__()`：增加时延梯度值的初始化
   - `store_episode()`：增加时延梯度值的存储
   - `sample()`：计算所有时延梯度值对应的概率，梯度值越大，概率越大；以概率随机取minibatch个经验
2. `runner.py`：调用 `env.step()` 时，同时返回时延梯度值；在保存经验时，同时保存时延梯度值
3. `env.py`：`env.step()`中，要计算方案的总时延梯度值：<font color=#FF0000>总时延关于各个计算资源分配值的梯度的总和</font>

<div align="center"><img src="https://fastly.jsdelivr.net/gh/wlchenGG/myPictureBed@main/images/20240516144047.png" alt="20240516144047" width="600px" align="center"/></div>

<div align="center"><img src="https://fastly.jsdelivr.net/gh/wlchenGG/myPictureBed@main/images/20240516144837.png" alt="20240516144837" width="600px" align="center"/></div>

##### FCSM_0516v02

过滤掉负值资源分配决策，直接置 0。

<div align="center"><img src="https://fastly.jsdelivr.net/gh/wlchenGG/myPictureBed@main/images/20240516182858.png" alt="20240516182858" width="500px" align="center"/></div>

<div align="center"><img src="https://fastly.jsdelivr.net/gh/wlchenGG/myPictureBed@main/images/20240516183437.png" alt="20240516183437" width="600px" align="center"/></div>

##### FCSM_0516v03

过滤负决策值，将原 `tanh` 激活函数产生的 `[-1,1]` 线性变换（+1再×0.5）到 `[0,1]`。

<div align="center"><img src="https://fastly.jsdelivr.net/gh/wlchenGG/myPictureBed@main/images/20240516190311.png" alt="20240516190311" width="700px" align="center"/></div>
<div align="center"><img src="https://fastly.jsdelivr.net/gh/wlchenGG/myPictureBed@main/images/20240516190129.png" alt="20240516190129" width="600px" align="center"/></div>

##### FCSM_0517v01

改成 softmax，不过滤负值

<div align="center"><img src="https://fastly.jsdelivr.net/gh/wlchenGG/myPictureBed@main/images/20240517235348.png" alt="20240517235348" width="600px" align="center"/></div>

<div align="center"><img src="https://fastly.jsdelivr.net/gh/wlchenGG/myPictureBed@main/images/4d9c347f2b248b9745ebc0149871973.png" alt="4d9c347f2b248b9745ebc0149871973" width="600px" align="center"/></div>

<div align="center"><img src="https://fastly.jsdelivr.net/gh/wlchenGG/myPictureBed@main/images/20240518133756.png" alt="20240518133756" width="600px" align="center"/></div>

看起来下层损失在下降的样子，但这效率未免太低了点。下层奖励看起来都没咋提升过的样子。上层损失和奖励倒似乎还行

上层1000轮，下层对应20000轮，即每20次下层训练进行1次上层训练。
但下层1000轮左右的数据好像也还不错（毕竟其实上下层网络结构没有大差别），我在想会不会是下层迭代过多了。
是不是可以
1.  让下层也只20次训练1次，其中前19次只迭代不训练，最后训练1次。 这样上下层的训练次数就相当。
或者
2.  下层每轮训练，但只训练到1000轮左右，之后下层就不再训练，只为上层提供卸载和分配决策，而上层继续训练，直到1000轮。
或者：
3.  因为下层状态动作空间比上层要大的多，那下层网络结构是不是也调复杂或规模大一些。

##### FCSM_0518v01 调整下层训练频次

1.  让下层也只20次训练1次，其中前19次只迭代不训练，最后训练1次。 这样上下层的训练次数就相当。
<div align="center"><img src="https://fastly.jsdelivr.net/gh/wlchenGG/myPictureBed@main/images/20240518180137.png" alt="20240518180137" width="600px" align="center"/></div>

2.  下层每轮训练，但只训练到1000轮左右，之后下层就不再训练，只为上层提供卸载和分配决策，而上层继续训练，直到1000轮。
<div align="center"><img src="https://fastly.jsdelivr.net/gh/wlchenGG/myPictureBed@main/images/20240519000148.png" alt="20240519000148" width="600px" align="center"/></div>

3.  因为下层状态动作空间比上层要大的多，那下层网络结构是不是也调复杂或规模大一些。增大了下层网络规模后的结果：
<div align="center"><img src="https://fastly.jsdelivr.net/gh/wlchenGG/myPictureBed@main/images/20240519000159.png" alt="20240519000159" width="600px" align="center"/></div>

##### FCSM_0519v01 调整时延梯度

总时延梯度越小，采样优先级越高

<div align="center"><img src="https://fastly.jsdelivr.net/gh/wlchenGG/myPictureBed@main/images/3c45cfef5f54cb346b72db36e7e4809.png" alt="3c45cfef5f54cb346b72db36e7e4809" width="300px" align="center"/></div>

<div align="center"><img src="https://fastly.jsdelivr.net/gh/wlchenGG/myPictureBed@main/images/91162dd4ae3f1cabc5e1232bd2942a0.png" alt="91162dd4ae3f1cabc5e1232bd2942a0" width="300px" align="center"/></div>


#### 代码

1. `main.py` 程序主函数：命令行执行 `python main.py` 即可运行整个强化学习算法。先创建强化学习的环境，然后创建 Runner 类。

2. `runner.py` 中（算法主体）：
   1. In `run()` <font color=#FF0000>算法主函数</font>：
      1. 按周期处理上层：
         - 
         - ActionUpper网络前向传播获取上层动作（服务部署）
         - ==问题：服务和微服务如何处理？参考matlab来？== <font color=#00AF00>不考虑微服务，只考虑不可分的服务（服务大小、计算负载等属性）</font>
      2. 按 Word 里算法思路，为下层增加一层循环，迭代 $\tau$ 个时隙，每个时隙处理：
         - 获取下层观察（基站已部署(微)服务、基站剩余存储/计算资源、协作终端已部署微服务、协作终端计算资源、请求服务各项值、用户位置、覆盖情况、<font color=#FF0000>上层输出的服务部署决策动作</font>）
         - ActionLower网络前向传播获取下层动作（计算卸载、计算资源分配）：==如何考虑时延梯度？从Action网络内还是外？损失函数上入手？== <font color=#00AF00>增加一个时延梯度向量作为附加启发式信息输入给网络</font>
         - 执行动作（部署、卸载、资源），观测下一时刻状态、奖励，并记入下层回放池
         - 如果下层回放池经验足够，则计算下层损失，更新下层 DDPG 参数。==同时是否需要进行参数共享和参数聚合== <font color=#00AF00>是上下层一起共享和聚合，因此应该上层经过周期次参数更新后</font>
         - 递增上层奖励
         - <u>记入上层回放池</u>
      3. 将下一时刻上层状态、$\tau$平均上层奖励计入上层回放池。==其中上层状态来自于下层执行 $\tau$ 后的观察，是否合理？毕竟上层所做的部署决策是基于 $t$ 时的状态，而后又经过 $\tau$ 个时隙的下层迭代后，得到的下一状态（实际是$t+\tau$后的状态）。**好像是没问题的。只是需要考虑如下图中问题：**==
      4. 如果上层回放池经验足够，则计算上层损失，更新下层 DDPG 参数
      5. ==是否需要进行参数共享和参数聚合？共享和聚合的周期分别为多少？目前简化流程，可以暂时不考虑共享，只在每次上层更新时，进行上下层的参数聚合== <font color=#00AF00>每经过一定次数的上层训练后，就执行一次类内参数共享，同时全局参数聚合。</font>
      6. 保存奖励和损失时间序列时，应该将上下层都保存，到时可以体现上下层的收敛情况。
   2. In `evaluate` ：~~次要，之后看看是否需要上下层都评价并记录 rewards~~

3. `env.py` 环境类：
   1. In `__init__()` 环境构造函数：
   2. In `reset()` 环境重置函数：
   3. In `step()` 环境步进函数：基于当前状态和动作，返回下一状态和奖励
      - 去掉 基站开关状态切换成本 相关
      - 去掉 服务迁移 相关
      - 更新 时延计算方法：==端协作时，数据传输时延如何考虑？通过覆盖基站转发还是终端 D2D 直连==
      - 统计各项指标：时延、存储资源、计算资源利用率、请求满足率、网络吞吐量
      - 汇总执行动作后的 下一步状态向量、奖励向量
4. `agent.py` 代理类：
   1. `__init__()`：已经改为初始化两个DDPG，`policy_upper` 用于上层，`policy_lower` 用于下层
   2. `softmax()`：不变
   3. `select_action()`：由于动作处理不一样，分成上层和下层函数
      1. `select_upper_action()`：动作定界较简单，动作 `pi` 大于 0.5 时决策就设为 1，否则决策设为 0。
      2. `select_lower_action()`：每个请求只能由覆盖基站和范围内协同终端之一提供卸载服务。==TODO:这个设定可能需要调整神经网络架构==
   4. `learn()`：不变
5. `maddpg.py`：
   1. `__init__()`：根据上下层指示参数，初始化上下层 actor 和 critic 网络
   2. `_soft_update_target_network()`：不变
   3. `train()`：不变
   4. `save_model()`：保存文件名标识上下层
6. `actor_critic.py` 网络结构类：分成上下层
   1. `class ActorUpper`：
   2. `class CriticUpper`：
   3. `class ActorLower`：
   4. `class CriticLower`：
7. `commmon/replay_buffer.py` 经验回放池类：
   1. `__init__()`：因为上下层的动作状态大小不同，因此区分上下层初始化
   2. `store_episode()`：不变
   3. `sample()`：不变
   4. `_get_storage_idx()`：不变
8. `commmon/arguments.py` 超参数设置：
   1. `get_args()`：增加上层训练周期参数 episode-upper=20，上层训练 batch-size-upper=16，因为下层是 256，尽量让两层训练频次接近
9.  `utils.py` 工具函数：
   1. [x] `make_env()` 创建环境函数：区分设定上下层 观测和动作长度
      ``` 
          # 环境状态：每个基站范围内的 任务总大小 + 各任务大小 + 各计算强度 + 各计算容量 + 用户位置 + 协作用户部署服务
                                      1 100 100 100 100 2*100 10
          # 对于每个代理：
          # Upper观察：每个基站范围内的 任务总大小 + 各任务大小 + 各计算强度 + 各计算容量 + 各部署服务 + 用户位置 + 协作用户部署服务
          #                           1 100 100 100 100 2*100 10
          # Lower观察：每个基站范围内的 任务总大小 + 各任务大小 + 各计算强度 + 各计算容量 + 各部署服务 + 用户位置  + 协作用户部署服务 + Upper动作
          #                           1 100 100 100 100 2*100 10 100
          
          # Upper动作：服务部署决策动作（假设每个用户对应一种服务）{0,1}
          #                           100
          # Lower动作：计算卸载决策动作 {0,1} + 10个终端协作卸载决策 {0,1}、资源分配决策动作 [0,1]
          #                           11*100 + 100
      ```
